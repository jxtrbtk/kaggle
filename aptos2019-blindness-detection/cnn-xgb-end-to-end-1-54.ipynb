{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "![CNN Schema](https://raw.githubusercontent.com/jxtrbtk/kaggle/master/aptos2019-blindness-detection/CNNSchema.png)\n",
    "\n",
    "This kernel is a playground around the following idea. CNN are made of two part, convultions that acts like a features extractor and a fully connected neural network used for classification. What if we replace the neural network classifier by an XGBoost based one ? XGBoost is most often very efficient and makes it easy to achieve very good performances without a lot of tuning effort.\n",
    "\n",
    "At the very beginning, I had better results with XGB, but the more optimized the process, the more similar the performance were...\n",
    "Hope you'll enjoy the journey !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports, settings and references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import time\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import quantile_transform\n",
    "\n",
    "import pickle\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\")\n",
    "DATA_SOURCE = os.path.join(\"..\",\"input\",\"aptos2019-blindness-detection\")\n",
    "MODEL_SOURCE = os.path.join(\"..\",\"input\",\"densenet161-1-18-v2-pth\")\n",
    "MODEL_SIZE = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(img,tol=7):\n",
    "    w, h = img.shape[1],img.shape[0]\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    gray_img = cv2.blur(gray_img,(5,5))\n",
    "    shape = gray_img.shape \n",
    "    gray_img = gray_img.reshape(-1,1)\n",
    "    quant = quantile_transform(gray_img, n_quantiles=256, random_state=0, copy=True)\n",
    "    quant = (quant*256).astype(int)\n",
    "    gray_img = quant.reshape(shape)\n",
    "    xp = (gray_img.mean(axis=0)>tol)\n",
    "    yp = (gray_img.mean(axis=1)>tol)\n",
    "    x1, x2 = np.argmax(xp), w-np.argmax(np.flip(xp))\n",
    "    y1, y2 = np.argmax(yp), h-np.argmax(np.flip(yp))\n",
    "    if x1 >= x2 or y1 >= y2 : # something wrong with the crop\n",
    "        return img # return original image\n",
    "    else:\n",
    "        img1=img[y1:y2,x1:x2,0]\n",
    "        img2=img[y1:y2,x1:x2,1]\n",
    "        img3=img[y1:y2,x1:x2,2]\n",
    "        img = np.stack([img1,img2,img3],axis=-1)\n",
    "    return img\n",
    "\n",
    "def process_image(image, size=512):\n",
    "    image = cv2.resize(image, (size,int(size*image.shape[0]/image.shape[1])))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    try:\n",
    "        image = crop_image(image, tol=15)\n",
    "    except Exception as e:\n",
    "        image = image\n",
    "        print( str(e) )\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*inspired by: https://www.kaggle.com/ratthachat/aptos-updated-preprocessing-ben-s-cropping*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch's style data loader defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "class RetinopathyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, transform, is_test=False):\n",
    "        self.transform = transform\n",
    "        self.base_transform = transforms.Resize((MODEL_SIZE, MODEL_SIZE))\n",
    "        self.is_test = is_test \n",
    "        if not os.path.exists(\"cache\"): os.mkdir(\"cache\")\n",
    "        if is_test : file = \"test.csv\"\n",
    "        else : file = \"train.csv\"\n",
    "        csv_file = os.path.join(DATA_SOURCE, file)\n",
    "        df = pd.read_csv(csv_file)\n",
    "        self.data = df.reset_index(drop=True)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_test : archive = \"test_images\"\n",
    "        else : archive = \"train_images\"\n",
    "        folder = os.path.join(DATA_SOURCE, archive)\n",
    "        code = str(self.data.loc[idx, 'id_code'])\n",
    "        file = code + \".png\"\n",
    "        cache_path = os.path.join(\"cache\",code+\".png\")\n",
    "        cached = os.path.exists(cache_path)\n",
    "        if not cached : \n",
    "            path = os.path.join(folder, file)\n",
    "            image = cv2.imread(path)\n",
    "            image = process_image(image)\n",
    "            imgpil = Image.fromarray(image)\n",
    "            imgpil = self.base_transform(imgpil)\n",
    "            imgpil.save(cache_path,\"PNG\")\n",
    "        imgpil = Image.open(cache_path)\n",
    "        img_tensor = self.transform(imgpil)\n",
    "        if self.is_test : return {'image': img_tensor} \n",
    "        else : \n",
    "            label = self.data.loc[idx, \"diagnosis\"]\n",
    "            return {'image': img_tensor, 'label': label}\n",
    "        \n",
    "\n",
    "    def get_df(self):\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*adapted from: https://www.kaggle.com/abhishek/very-simple-pytorch-training-0-59*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-train the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we will prepare the dataset and create the folds\n",
    "NUM_FOLDS = 5\n",
    "\n",
    "data_augmentation = transforms.Compose([\n",
    "    transforms.RandomRotation((-15, 15)),\n",
    "    transforms.Resize(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "DATA = RetinopathyDataset(data_augmentation)\n",
    "df = DATA.get_df()\n",
    "skf = StratifiedKFold(n_splits=NUM_FOLDS)\n",
    "folds_generator = skf.split(df.index.values, df.diagnosis.values)\n",
    "data_train, data_eval = [], [] \n",
    "for t, e in folds_generator:\n",
    "    data_train.append(t)\n",
    "    data_eval.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader_for_fold(n, data, train_data, eval_data, batch_size):\n",
    "    \"\"\" return the train and eval dataloader for a fold\n",
    "    \"\"\"    \n",
    "    train_sampler = SubsetRandomSampler(train_data[n])\n",
    "    valid_sampler = SubsetRandomSampler(eval_data[n])\n",
    "\n",
    "    data_loader_train = torch.utils.data.DataLoader(data, \n",
    "                    batch_size=batch_size, drop_last=False, \n",
    "                    sampler=train_sampler)\n",
    "    data_loader_eval = torch.utils.data.DataLoader(data, \n",
    "                    batch_size=batch_size, drop_last=False, \n",
    "                    sampler=valid_sampler)\n",
    "    \n",
    "    return data_loader_train, data_loader_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classificator0(nn.Module):\n",
    "    \"\"\" classifier layer used to retrain the CNN\n",
    "    \"\"\"    \n",
    "    def __init__(self, size=128):\n",
    "        super(Classificator0, self).__init__()\n",
    "        self.size = size\n",
    "        self.network = nn.Sequential(\n",
    "              nn.BatchNorm1d(size),\n",
    "              nn.Dropout(p=0.3),\n",
    "              nn.Linear(in_features=size, out_features=5, bias=True),\n",
    "        )        \n",
    "    def forward(self, x):\n",
    "        ## Define forward behavior\n",
    "        return self.network(x)\n",
    "\n",
    "class Classificator(nn.Module):\n",
    "    \"\"\" classifier layer used to retrain the CNN\n",
    "    \"\"\"    \n",
    "    def __init__(self, size=128):\n",
    "        super(Classificator, self).__init__()\n",
    "        self.size = size\n",
    "        self.network = nn.Sequential(\n",
    "              nn.BatchNorm1d(size),\n",
    "              nn.Dropout(p=0.25),\n",
    "              nn.Linear(in_features=size, out_features=size, bias=True),\n",
    "              nn.ReLU(),\n",
    "              nn.BatchNorm1d(size),\n",
    "              nn.Dropout(p=0.5),\n",
    "              nn.Linear(in_features=size, out_features=5, bias=True),\n",
    "    )        \n",
    "    def forward(self, x):\n",
    "        ## Define forward behavior\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_model():\n",
    "    \"\"\" get the pretrained model\n",
    "    \"\"\"    \n",
    "    model = torchvision.models.densenet161(pretrained=False)\n",
    "    in_features = model.classifier.in_features\n",
    "    model.classifier = Classificator0(in_features)\n",
    "    model_path = os.path.join(MODEL_SOURCE, \"densenet161.1.18.v2.pth\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.classifier = Classificator(in_features)\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, scheduler, train_data_loader, eval_data_loader, \n",
    "                file_name, num_epochs = 50, patience = 7, prev_loss = 1000.00):\n",
    "    \"\"\" train the model\n",
    "    arguments : model, optimizer, scheduler, train_data_loader, eval_data_loader\n",
    "        file_name: name of the file to save the best model \n",
    "        num_epochs: maximum number of epochs\n",
    "        patience: number of epochs to wait if no improvements\n",
    "        prev_loss: previous loss achieved, to surpass to have the model saved\n",
    "    return: \n",
    "        best loss achieved (previous loss if not surpassed)\n",
    "    \"\"\"    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    countdown = patience\n",
    "    best_loss = 1000.00\n",
    "    since = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        counter = 0\n",
    "        for bi, d in enumerate(train_data_loader):\n",
    "            inputs = d[\"image\"].to(DEVICE, dtype=torch.float)\n",
    "            labels = d[\"label\"].to(DEVICE, dtype=torch.long)\n",
    "            # batch norm layers needs more than 1 set of data\n",
    "            # this is to skip the last batch if it's only 1 image\n",
    "            if inputs.shape[0] > 1 :\n",
    "                counter += inputs.size(0)\n",
    "                model.to(DEVICE)\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs) \n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                loss_val = running_loss / counter\n",
    "                print(\"{:7} {:.4f} {:.4f}\".format(counter, loss.item()*1, loss_val), end=\"\\r\")\n",
    "        epoch_loss = running_loss / ( len(train_data_loader) * train_data_loader.batch_size)\n",
    "        time_elapsed = time.time() - since\n",
    "        print(\" T{:3}/{:3} loss: {:.4f} ({:3.0f}m {:2.0f}s)\".format( \n",
    "            epoch, num_epochs - 1, epoch_loss,time_elapsed // 60, time_elapsed % 60))\n",
    "        running_loss = 0.0\n",
    "        counter = 0\n",
    "        for bi, d in enumerate(eval_data_loader):\n",
    "            inputs = d[\"image\"].to(DEVICE, dtype=torch.float)\n",
    "            counter += inputs.size(0)\n",
    "            labels = d[\"label\"].to(DEVICE, dtype=torch.long)\n",
    "            model.to(DEVICE)\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            loss_val = running_loss / counter\n",
    "            print(\"{:7} {:.4f} {:.4f}\".format(counter, loss.item()*1, loss_val), end=\"\\r\")\n",
    "        epoch_loss = running_loss / ( len(eval_data_loader) * eval_data_loader.batch_size)\n",
    "        if epoch_loss < best_loss : \n",
    "            best_loss = epoch_loss\n",
    "            if epoch_loss < prev_loss:\n",
    "                torch.save(model.state_dict(), file_name)\n",
    "                prev_loss = epoch_loss\n",
    "                print(\"*\", end=\"\")\n",
    "            else:\n",
    "                print(\".\", end=\"\")\n",
    "            countdown = patience\n",
    "        else:\n",
    "            print(\"{:1}\".format(countdown), end=\"\")\n",
    "            countdown -= 1\n",
    "        time_elapsed = time.time() - since\n",
    "        print(\"E{:3}/{:3} loss: {:.4f} ({:3.0f}m {:2.0f}s)\".format( \n",
    "            epoch, num_epochs - 1, epoch_loss,time_elapsed // 60, time_elapsed % 60 ))\n",
    "        scheduler.step() #epoch_loss\n",
    "\n",
    "        if countdown <= 0 : break\n",
    "\n",
    "    return prev_loss\n",
    "    print(\"done.\")\n",
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- fold 0\n",
      "----------- round 0\n",
      " T  0/ 12 loss: 0.5055 (  6m 31s)\n",
      "*E  0/ 12 loss: 0.3964 (  8m  8s)\n",
      " T  1/ 12 loss: 0.3536 (  8m 55s)\n",
      "*E  1/ 12 loss: 0.3438 (  9m  1s)\n",
      " T  2/ 12 loss: 0.3451 (  9m 48s)\n",
      "*E  2/ 12 loss: 0.3359 (  9m 53s)\n",
      " T  3/ 12 loss: 0.3192 ( 10m 40s)\n",
      "3E  3/ 12 loss: 0.3436 ( 10m 46s)\n",
      " T  4/ 12 loss: 0.3120 ( 11m 32s)\n",
      "*E  4/ 12 loss: 0.3283 ( 11m 38s)\n",
      " T  5/ 12 loss: 0.2947 ( 12m 25s)\n",
      "3E  5/ 12 loss: 0.3300 ( 12m 31s)\n",
      " T  6/ 12 loss: 0.2931 ( 13m 17s)\n",
      "*E  6/ 12 loss: 0.3204 ( 13m 23s)\n",
      " T  7/ 12 loss: 0.2970 ( 14m 10s)\n",
      "*E  7/ 12 loss: 0.3190 ( 14m 16s)\n",
      " T  8/ 12 loss: 0.2876 ( 15m  3s)\n",
      "3E  8/ 12 loss: 0.3257 ( 15m  8s)\n",
      " T  9/ 12 loss: 0.2839 ( 15m 55s)\n",
      "2E  9/ 12 loss: 0.3319 ( 16m  1s)\n",
      " T 10/ 12 loss: 0.2999 ( 16m 47s)\n",
      "1E 10/ 12 loss: 0.3297 ( 16m 53s)\n",
      "----------- round 1\n",
      " T  0/ 12 loss: 0.5112 (  0m 47s)\n",
      ".E  0/ 12 loss: 0.4181 (  0m 52s)\n",
      " T  1/ 12 loss: 0.3660 (  1m 39s)\n",
      ".E  1/ 12 loss: 0.3507 (  1m 45s)\n",
      " T  2/ 12 loss: 0.3474 (  2m 32s)\n",
      ".E  2/ 12 loss: 0.3409 (  2m 37s)\n",
      " T  3/ 12 loss: 0.3342 (  3m 24s)\n",
      "3E  3/ 12 loss: 0.3460 (  3m 30s)\n",
      " T  4/ 12 loss: 0.3221 (  4m 16s)\n",
      "2E  4/ 12 loss: 0.3445 (  4m 22s)\n",
      " T  5/ 12 loss: 0.3032 (  5m  9s)\n",
      ".E  5/ 12 loss: 0.3339 (  5m 14s)\n",
      " T  6/ 12 loss: 0.2984 (  6m  1s)\n",
      "3E  6/ 12 loss: 0.3417 (  6m  7s)\n",
      " T  7/ 12 loss: 0.3020 (  6m 54s)\n",
      "2E  7/ 12 loss: 0.3358 (  6m 59s)\n",
      " T  8/ 12 loss: 0.2981 (  7m 46s)\n",
      "1E  8/ 12 loss: 0.3368 (  7m 52s)\n",
      "---------------------- best loss 0.31903610690211764\n",
      "\n",
      "---------------------- fold 1\n",
      "----------- round 0\n",
      " T  0/ 12 loss: 0.5221 (  0m 47s)\n",
      "*E  0/ 12 loss: 0.3451 (  0m 53s)\n",
      " T  1/ 12 loss: 0.3638 (  1m 39s)\n",
      "*E  1/ 12 loss: 0.3331 (  1m 45s)\n",
      " T  2/ 12 loss: 0.3479 (  2m 32s)\n",
      "*E  2/ 12 loss: 0.3327 (  2m 38s)\n",
      " T  3/ 12 loss: 0.3262 (  3m 25s)\n",
      "*E  3/ 12 loss: 0.3215 (  3m 30s)\n",
      " T  4/ 12 loss: 0.3128 (  4m 17s)\n",
      "3E  4/ 12 loss: 0.3217 (  4m 23s)\n",
      " T  5/ 12 loss: 0.2915 (  5m 10s)\n",
      "*E  5/ 12 loss: 0.3199 (  5m 16s)\n",
      " T  6/ 12 loss: 0.2896 (  6m  2s)\n",
      "3E  6/ 12 loss: 0.3232 (  6m  8s)\n",
      " T  7/ 12 loss: 0.3026 (  6m 55s)\n",
      "2E  7/ 12 loss: 0.3329 (  7m  1s)\n",
      " T  8/ 12 loss: 0.3007 (  7m 47s)\n",
      "1E  8/ 12 loss: 0.3317 (  7m 53s)\n",
      "----------- round 1\n",
      " T  0/ 12 loss: 0.5197 (  0m 47s)\n",
      ".E  0/ 12 loss: 0.3721 (  0m 53s)\n",
      " T  1/ 12 loss: 0.3559 (  1m 40s)\n",
      ".E  1/ 12 loss: 0.3478 (  1m 46s)\n",
      " T  2/ 12 loss: 0.3369 (  2m 33s)\n",
      ".E  2/ 12 loss: 0.3415 (  2m 39s)\n",
      " T  3/ 12 loss: 0.3357 (  3m 26s)\n",
      ".E  3/ 12 loss: 0.3408 (  3m 32s)\n",
      " T  4/ 12 loss: 0.3265 (  4m 19s)\n",
      "3E  4/ 12 loss: 0.3457 (  4m 24s)\n",
      " T  5/ 12 loss: 0.2909 (  5m 12s)\n",
      "2E  5/ 12 loss: 0.3439 (  5m 17s)\n",
      " T  6/ 12 loss: 0.2925 (  6m  5s)\n",
      ".E  6/ 12 loss: 0.3293 (  6m 10s)\n",
      " T  7/ 12 loss: 0.3034 (  6m 57s)\n",
      ".E  7/ 12 loss: 0.3252 (  7m  3s)\n",
      " T  8/ 12 loss: 0.2842 (  7m 50s)\n",
      "3E  8/ 12 loss: 0.3362 (  7m 56s)\n",
      " T  9/ 12 loss: 0.2969 (  8m 43s)\n",
      "2E  9/ 12 loss: 0.3298 (  8m 49s)\n",
      " T 10/ 12 loss: 0.2865 (  9m 36s)\n",
      "1E 10/ 12 loss: 0.3383 (  9m 42s)\n",
      "---------------------- best loss 0.31994242069064355\n",
      "\n",
      "---------------------- fold 2\n",
      "----------- round 0\n",
      " T  0/ 12 loss: 0.5182 (  0m 47s)\n",
      "*E  0/ 12 loss: 0.4101 (  0m 53s)\n",
      " T  1/ 12 loss: 0.3782 (  1m 40s)\n",
      "*E  1/ 12 loss: 0.3417 (  1m 46s)\n",
      " T  2/ 12 loss: 0.3472 (  2m 34s)\n",
      "*E  2/ 12 loss: 0.3166 (  2m 40s)\n",
      " T  3/ 12 loss: 0.3333 (  3m 27s)\n",
      "3E  3/ 12 loss: 0.3295 (  3m 32s)\n",
      " T  4/ 12 loss: 0.3214 (  4m 20s)\n",
      "2E  4/ 12 loss: 0.3166 (  4m 25s)\n",
      " T  5/ 12 loss: 0.2942 (  5m 12s)\n",
      "1E  5/ 12 loss: 0.3238 (  5m 18s)\n",
      "----------- round 1\n",
      " T  0/ 12 loss: 0.5116 (  0m 47s)\n",
      ".E  0/ 12 loss: 0.3261 (  0m 53s)\n",
      " T  1/ 12 loss: 0.3608 (  1m 40s)\n",
      "3E  1/ 12 loss: 0.3306 (  1m 46s)\n",
      " T  2/ 12 loss: 0.3330 (  2m 33s)\n",
      "2E  2/ 12 loss: 0.3357 (  2m 39s)\n",
      " T  3/ 12 loss: 0.3329 (  3m 26s)\n",
      "*E  3/ 12 loss: 0.3112 (  3m 32s)\n",
      " T  4/ 12 loss: 0.3110 (  4m 19s)\n",
      "3E  4/ 12 loss: 0.3311 (  4m 25s)\n",
      " T  5/ 12 loss: 0.2915 (  5m 12s)\n",
      "2E  5/ 12 loss: 0.3311 (  5m 17s)\n",
      " T  6/ 12 loss: 0.2953 (  6m  5s)\n",
      "1E  6/ 12 loss: 0.3211 (  6m 10s)\n",
      "---------------------- best loss 0.31118273484159487\n",
      "\n",
      "---------------------- fold 3\n",
      "----------- round 0\n",
      " T  0/ 12 loss: 0.5195 (  0m 47s)\n",
      "*E  0/ 12 loss: 0.3965 (  0m 53s)\n",
      " T  1/ 12 loss: 0.3486 (  1m 40s)\n",
      "*E  1/ 12 loss: 0.3503 (  1m 46s)\n",
      " T  2/ 12 loss: 0.3419 (  2m 33s)\n",
      "*E  2/ 12 loss: 0.3502 (  2m 39s)\n",
      " T  3/ 12 loss: 0.3195 (  3m 26s)\n",
      "*E  3/ 12 loss: 0.3369 (  3m 32s)\n",
      " T  4/ 12 loss: 0.3120 (  4m 20s)\n",
      "*E  4/ 12 loss: 0.3364 (  4m 25s)\n",
      " T  5/ 12 loss: 0.2911 (  5m 13s)\n",
      "3E  5/ 12 loss: 0.3410 (  5m 18s)\n",
      " T  6/ 12 loss: 0.2932 (  6m  6s)\n",
      "2E  6/ 12 loss: 0.3519 (  6m 11s)\n",
      " T  7/ 12 loss: 0.2778 (  6m 58s)\n",
      "1E  7/ 12 loss: 0.3387 (  7m  4s)\n",
      "----------- round 1\n",
      " T  0/ 12 loss: 0.4952 (  0m 47s)\n",
      ".E  0/ 12 loss: 0.3780 (  0m 53s)\n",
      " T  1/ 12 loss: 0.3495 (  1m 40s)\n",
      ".E  1/ 12 loss: 0.3509 (  1m 46s)\n",
      " T  2/ 12 loss: 0.3293 (  2m 33s)\n",
      "3E  2/ 12 loss: 0.3514 (  2m 39s)\n",
      " T  3/ 12 loss: 0.3263 (  3m 26s)\n",
      "2E  3/ 12 loss: 0.3581 (  3m 31s)\n",
      " T  4/ 12 loss: 0.3125 (  4m 19s)\n",
      ".E  4/ 12 loss: 0.3389 (  4m 24s)\n",
      " T  5/ 12 loss: 0.2975 (  5m 12s)\n",
      "3E  5/ 12 loss: 0.3411 (  5m 17s)\n",
      " T  6/ 12 loss: 0.3023 (  6m  4s)\n",
      "2E  6/ 12 loss: 0.3413 (  6m 10s)\n",
      " T  7/ 12 loss: 0.2879 (  6m 57s)\n",
      "*E  7/ 12 loss: 0.3359 (  7m  3s)\n",
      " T  8/ 12 loss: 0.2991 (  7m 50s)\n",
      "3E  8/ 12 loss: 0.3384 (  7m 56s)\n",
      " T  9/ 12 loss: 0.2796 (  8m 43s)\n",
      "2E  9/ 12 loss: 0.3481 (  8m 49s)\n",
      " T 10/ 12 loss: 0.2800 (  9m 36s)\n",
      "1E 10/ 12 loss: 0.3400 (  9m 42s)\n",
      "---------------------- best loss 0.3359153941273689\n",
      "\n",
      "---------------------- fold 4\n",
      "----------- round 0\n",
      " T  0/ 12 loss: 0.5091 (  0m 47s)\n",
      "*E  0/ 12 loss: 0.4416 (  0m 53s)\n",
      " T  1/ 12 loss: 0.3520 (  1m 40s)\n",
      "*E  1/ 12 loss: 0.3944 (  1m 46s)\n",
      " T  2/ 12 loss: 0.3302 (  2m 33s)\n",
      "3E  2/ 12 loss: 0.4043 (  2m 39s)\n",
      " T  3/ 12 loss: 0.3113 (  3m 26s)\n",
      "2E  3/ 12 loss: 0.3996 (  3m 32s)\n",
      " T  4/ 12 loss: 0.3122 (  4m 19s)\n",
      "1E  4/ 12 loss: 0.3971 (  4m 25s)\n",
      "----------- round 1\n",
      " T  0/ 12 loss: 0.5203 (  0m 47s)\n",
      ".E  0/ 12 loss: 0.4034 (  0m 53s)\n",
      " T  1/ 12 loss: 0.3526 (  1m 40s)\n",
      ".E  1/ 12 loss: 0.3966 (  1m 46s)\n",
      " T  2/ 12 loss: 0.3226 (  2m 33s)\n",
      ".E  2/ 12 loss: 0.3946 (  2m 39s)\n",
      " T  3/ 12 loss: 0.3082 (  3m 26s)\n",
      "*E  3/ 12 loss: 0.3771 (  3m 32s)\n",
      " T  4/ 12 loss: 0.3105 (  4m 19s)\n",
      "3E  4/ 12 loss: 0.3913 (  4m 25s)\n",
      " T  5/ 12 loss: 0.2864 (  5m 12s)\n",
      "2E  5/ 12 loss: 0.3867 (  5m 17s)\n",
      " T  6/ 12 loss: 0.2794 (  6m  5s)\n",
      "1E  6/ 12 loss: 0.4058 (  6m 10s)\n",
      "---------------------- best loss 0.37710094543135897\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train the model num_round_per_fold times for each fold\n",
    "# and the save the best model for each fold\n",
    "batch_size = 56\n",
    "num_round_per_fold = 2\n",
    "for no in range(NUM_FOLDS):\n",
    "    print(\"-\"*22, \"fold\",no)\n",
    "    bst_loss = 10000.00\n",
    "    for r in range(num_round_per_fold):\n",
    "        print(\"-\"*11,\"round\",r)\n",
    "        data_loader_train, data_loader_eval = get_dataloader_for_fold(no, \n",
    "                                    DATA, data_train, data_eval, batch_size)\n",
    "        model = get_base_model()\n",
    "        plist = [{\"params\": model.features.denseblock4.parameters(), \"lr\":0.0001},\n",
    "                 {\"params\": model.classifier.parameters()}]\n",
    "        optimizer = optim.Adam(plist, lr=0.001, amsgrad=True)\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [1,5], gamma=0.1, last_epoch=-1)        \n",
    "        bst_loss = train_model(model, optimizer, scheduler, \n",
    "                               data_loader_train, data_loader_eval, \n",
    "                               \"tmp\"+str(no)+\".pth\", prev_loss=bst_loss, \n",
    "                               num_epochs=13, patience=3)\n",
    "    print(\"-\"*22, \"best loss\", bst_loss)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract train features from CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trained_model(no): \n",
    "    \"\"\" reload and return the retrained model for the given fold \n",
    "    \"\"\"    \n",
    "    extractor = torchvision.models.densenet161(pretrained=False)\n",
    "    in_features = extractor.classifier.in_features\n",
    "    extractor.classifier = Classificator(in_features)\n",
    "    model_path = os.path.join(\"tmp\"+str(no)+\".pth\") #no\n",
    "    extractor.load_state_dict(torch.load(model_path))\n",
    "    extractor = extractor.to(DEVICE)\n",
    "    extractor.eval()\n",
    "    return extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extractor_model(no):\n",
    "    \"\"\" reload and return the retrained model for the given fold\n",
    "        and make last layer identity\n",
    "    \"\"\"    \n",
    "    extractor = get_trained_model(no)\n",
    "    extractor.classifier = nn.Identity()\n",
    "    extractor = extractor.to(DEVICE)\n",
    "    extractor.eval()\n",
    "    return extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_features(data_loader, extractor):\n",
    "    \"\"\" return 2 arrays of features extracted, and targets  \n",
    "    \"\"\"    \n",
    "    for bi, d in enumerate(data_loader):\n",
    "        print(\".\", end=\"\")\n",
    "        img_tensor = d[\"image\"].to(DEVICE)\n",
    "        target = d[\"label\"].numpy()\n",
    "        with torch.no_grad(): feature = extractor(img_tensor)\n",
    "        feature = feature.cpu().detach().squeeze(0).numpy()\n",
    "        if bi == 0 :\n",
    "            features = feature \n",
    "            targets = target \n",
    "        else :\n",
    "            features = np.concatenate([features, feature], axis=0)\n",
    "            targets = np.concatenate([targets, target], axis=0)\n",
    "\n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBOOST_PARAM = {\n",
    "    \"random_state\"      : 42,\n",
    "    \"n_estimators\"      : 200,\n",
    "    \"objective\"         : \"multi:softmax\",\n",
    "    \"num_class\"         : 5,\n",
    "    \"eval_metric\"       : \"mlogloss\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- fold 0\n",
      "...........|.............................................|\n",
      "..........................................................\n",
      "score 0.398695\n",
      "---------------------- fold 1\n",
      "...........|.............................................|\n",
      "..........................................................\n",
      "score 0.358689\n",
      "---------------------- fold 2\n",
      "...........|.............................................|\n",
      "..........................................................\n",
      "score 0.370593\n",
      "---------------------- fold 3\n",
      "...........|.............................................|\n",
      "..........................................................\n",
      "score 0.38977\n",
      "---------------------- fold 4\n",
      "...........|.............................................|\n",
      "..........................................................\n",
      "score 0.434534\n"
     ]
    }
   ],
   "source": [
    "# for each fold, get the data loader, extractor, \n",
    "# extract the features (loaded and process each time, but we can have data aug.) \n",
    "# calcul the weights table, create and fit the XGB model\n",
    "batch_size = 64\n",
    "eval_set = []\n",
    "for no in range(NUM_FOLDS):\n",
    "    print(\"-\"*22, \"fold\",no)\n",
    "    data_loader_train, data_loader_eval = get_dataloader_for_fold(no, \n",
    "                                DATA, data_train, data_eval, batch_size)\n",
    "    extractor = get_extractor_model(no)\n",
    "\n",
    "    print(\"...........|.............................................|\")\n",
    "    features_eval, targets_eval = get_train_features(data_loader_eval,\n",
    "                                                     extractor)\n",
    "    features_train, targets_train = get_train_features(data_loader_train,\n",
    "                                                       extractor)\n",
    "    print(\"\")\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(**XGBOOST_PARAM)\n",
    "    xgb_model = xgb_model.fit(features_train,targets_train.reshape(-1),\n",
    "                        eval_set=[(features_eval, targets_eval.reshape(-1))],\n",
    "                        early_stopping_rounds=20,\n",
    "                        verbose=False)\n",
    "    print(\"score\",xgb_model.evals_result()[\"validation_0\"][\"mlogloss\"][-1])\n",
    "    pickle.dump(xgb_model, open(\"xgb_model_\"+str(no), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "## .using the full CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the data augmentation of the dataset object\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "DATA.transform = base_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen Kappa quadratic score 0.921920495505459\n"
     ]
    }
   ],
   "source": [
    "# first we make predictions using the CNN models for each fold \n",
    "# and calculate the score of all predictions\n",
    "predictions, targets = np.zeros(len(DATA)), np.zeros(len(DATA))\n",
    "batch_slice = (0, 0)\n",
    "softmax = nn.Softmax(dim=1)\n",
    "for no in range(NUM_FOLDS):\n",
    "    _, data_loader_eval = get_dataloader_for_fold(no, \n",
    "                DATA, data_train, data_eval, 64)\n",
    "    model = get_trained_model(no) #no\n",
    "    for bi, d in enumerate(data_loader_eval):\n",
    "        inputs = d[\"image\"].to(DEVICE, dtype=torch.float)\n",
    "        batch_slice = (batch_slice[1], batch_slice[1]+inputs.size(0))\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            outputs = softmax(outputs)\n",
    "        predictions[batch_slice[0]:batch_slice[1]] = \\\n",
    "                outputs.cpu().detach().squeeze(0).numpy().argmax(axis=1)\n",
    "        targets[batch_slice[0]:batch_slice[1]] = d[\"label\"]\n",
    "        \n",
    "print(\"Cohen Kappa quadratic score\", \n",
    "      cohen_kappa_score(targets, predictions, weights=\"quadratic\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .using the XGB models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............\n",
      "............\n",
      "............\n",
      "............\n",
      "............\n",
      "Cohen Kappa quadratic score 0.9141493864803752\n"
     ]
    }
   ],
   "source": [
    "# now let's do it with the XGB models \n",
    "predictions, targets = np.zeros(0), np.zeros(0)\n",
    "for no in range(NUM_FOLDS):\n",
    "    _, data_loader_eval = get_dataloader_for_fold(no, \n",
    "                                DATA, data_train, data_eval, batch_size)\n",
    "    features_eval, targets_eval = get_train_features(data_loader_eval,\n",
    "                                                     extractor)\n",
    "    print(\"\")\n",
    "    xgb_model = xgb.XGBClassifier()\n",
    "    model_path = os.path.join(\"xgb_model_\"+str(no))\n",
    "    xgb_model = pickle.load(open(model_path, \"rb\"))\n",
    "    prediction = xgb_model.predict(features_eval)\n",
    "    predictions = np.concatenate([predictions, prediction], axis=0)\n",
    "    targets = np.concatenate([targets, targets_eval], axis=0)\n",
    "\n",
    "print(\"Cohen Kappa quadratic score\", \n",
    "      cohen_kappa_score(targets, predictions, weights=\"quadratic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning\n",
    "if os.path.exists(\"cache\"):\n",
    "    for e in os.listdir(\"cache\"):\n",
    "        os.remove(os.path.join(\"cache\", e))\n",
    "    os.rmdir(\"cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract test features using CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "data_test = RetinopathyDataset(base_transform, is_test=True)\n",
    "data_loader = torch.utils.data.DataLoader(data_test, \n",
    "                            batch_size=16, shuffle=False, \n",
    "                            num_workers=0, drop_last=False)\n",
    "\n",
    "def get_test_features(data_loader, extractor):\n",
    "    \"\"\" return an array of features extracted  \n",
    "    \"\"\"    \n",
    "    for bi, d in enumerate(data_loader):\n",
    "        if bi % 4 == 0 : print(\".\", end=\"\")\n",
    "        img_tensor = d[\"image\"].to(DEVICE)\n",
    "        with torch.no_grad(): feature = extractor(img_tensor)\n",
    "        feature = feature.cpu().detach().numpy() #.squeeze(0) for batch_size > 1\n",
    "        if bi == 0 :\n",
    "            features = feature \n",
    "        else :\n",
    "            features = np.concatenate([features, feature], axis=0)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction using XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................ v\n",
      "............tta0................\n",
      "............................... 0\n",
      "............................... 1\n",
      "............................... 2\n",
      "............................... 3\n",
      "............................... 4\n"
     ]
    }
   ],
   "source": [
    "# adding prediction for each model \n",
    "# we can loop several time to perform data augmentation (tta) \n",
    "# (note: a bit risky as we cache the image, should be done bucket by bucket\n",
    "#  and clean after each bucket to avoid filling all the disk space)\n",
    "print(\"................................ v\")\n",
    "predictions = np.zeros((len(data_test),5))\n",
    "for tta in range(1):\n",
    "    print(\"............tta\"+str(tta)+\"................\")\n",
    "    for no in range(NUM_FOLDS):\n",
    "        extractor = get_extractor_model(no)\n",
    "        features = get_test_features(data_loader, extractor)\n",
    "        print(\"\",no)\n",
    "        xgb_model = xgb.XGBClassifier()\n",
    "        model_path = os.path.join(\"xgb_model_\"+str(no))\n",
    "        xgb_model = pickle.load(open(model_path, \"rb\"))\n",
    "        prediction = xgb_model.predict_proba(features)\n",
    "        predictions = predictions + prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "data_augmentation = transforms.Compose([\n",
    "    transforms.Resize((MODEL_SIZE, MODEL_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "data_test = RetinopathyDataset(data_augmentation, is_test=True)\n",
    "data_loader = torch.utils.data.DataLoader(data_test, \n",
    "                            batch_size=batch_size, shuffle=False, \n",
    "                            num_workers=0, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............tta0...............\n",
      "...............................\n",
      "...............................\n",
      "...............................\n",
      "...............................\n",
      "...............................\n",
      "............tta1...............\n",
      "...............................\n",
      "...............................\n",
      "...............................\n",
      "...............................\n",
      "...............................\n",
      "............tta2...............\n",
      "...............................\n",
      "...............................\n",
      "...............................\n",
      "...............................\n",
      "...............................\n",
      "............tta3...............\n",
      "...............................\n",
      "...............................\n",
      "...............................\n",
      "...............................\n",
      "...............................\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "for tta in range(4):\n",
    "    print(\"............tta\"+str(tta)+\"...............\")\n",
    "    for no in range(NUM_FOLDS):\n",
    "        model = get_trained_model(no) #no\n",
    "        batch_slice = (0, 0)\n",
    "        for bi, d in enumerate(data_loader):\n",
    "            if bi % (64//batch_size) == 0 : print(\".\", end=\"\")\n",
    "            img_tensor = d[\"image\"].to(DEVICE, dtype=torch.float)\n",
    "            batch_slice = (batch_slice[1], batch_slice[1]+img_tensor.size(0))\n",
    "            with torch.no_grad(): \n",
    "                feature = model(img_tensor)\n",
    "                feature = softmax(feature)\n",
    "                predictions[batch_slice[0]:batch_slice[1],:] += \\\n",
    "                        feature.cpu().detach().squeeze(0).numpy()\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voting\n",
    "prediction_final = predictions.argmax(axis=1)\n",
    "csv_file = os.path.join(DATA_SOURCE, \"sample_submission.csv\")\n",
    "df = pd.read_csv(csv_file)\n",
    "df[\"diagnosis\"] = prediction_final\n",
    "df.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0005cfc8afb6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003f0afdcd15</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006efc72b638</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00836aaacf06</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>009245722fa4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>009c019a7309</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>010d915e229a</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0111b949947e</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>01499815e469</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0167076e7089</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>01c31b10ab99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>01c5ba195207</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>01e4d86b3a30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>020921b796d5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>020f6983114d</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>021c207614d6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0229c0a80d42</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>024d0a225db1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0268f4382c67</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0299d97f31f7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>03042a663e54</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>030e06ddbb04</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>033cdbbbdfaa</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>03be80919be4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>03eaa4eef484</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0434995d0654</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>04a0773c71fb</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>04e1b77ef107</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>051d9d12a6ee</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>052e00f47cfa</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1898</th>\n",
       "      <td>fc66648f758e</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>fc67df1e574e</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>fc6c9d0efe53</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901</th>\n",
       "      <td>fc79feb5deed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902</th>\n",
       "      <td>fcbc1f4b5342</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1903</th>\n",
       "      <td>fcd166e6e4b5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>fcea00df9f33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905</th>\n",
       "      <td>fd2978398705</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906</th>\n",
       "      <td>fd2d58c6cd45</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1907</th>\n",
       "      <td>fd4d81b43e84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1908</th>\n",
       "      <td>fd7cc592106e</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909</th>\n",
       "      <td>fd8e6b0b2e45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1910</th>\n",
       "      <td>fda8612fcc8c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>fdde61dd1bde</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1912</th>\n",
       "      <td>fde8778182af</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913</th>\n",
       "      <td>fe0a340c4477</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1914</th>\n",
       "      <td>fe190d618acf</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>fe1d2f703efc</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>fe5618ad2460</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>fe57ff56618e</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>fe84ad1df04b</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>fe920e47b72d</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>fed9d587f158</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>fee5bd042c3b</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>fef8e645d030</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923</th>\n",
       "      <td>ff2fd94448de</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1924</th>\n",
       "      <td>ff4c945d9b17</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1925</th>\n",
       "      <td>ff64897ac0d8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1926</th>\n",
       "      <td>ffa73465b705</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927</th>\n",
       "      <td>ffdc2152d455</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1928 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id_code  diagnosis\n",
       "0     0005cfc8afb6          1\n",
       "1     003f0afdcd15          3\n",
       "2     006efc72b638          2\n",
       "3     00836aaacf06          2\n",
       "4     009245722fa4          2\n",
       "5     009c019a7309          2\n",
       "6     010d915e229a          3\n",
       "7     0111b949947e          0\n",
       "8     01499815e469          3\n",
       "9     0167076e7089          0\n",
       "10    01c31b10ab99          1\n",
       "11    01c5ba195207          2\n",
       "12    01e4d86b3a30          1\n",
       "13    020921b796d5          2\n",
       "14    020f6983114d          2\n",
       "15    021c207614d6          2\n",
       "16    0229c0a80d42          2\n",
       "17    024d0a225db1          2\n",
       "18    0268f4382c67          2\n",
       "19    0299d97f31f7          1\n",
       "20    03042a663e54          2\n",
       "21    030e06ddbb04          2\n",
       "22    033cdbbbdfaa          2\n",
       "23    03be80919be4          2\n",
       "24    03eaa4eef484          2\n",
       "25    0434995d0654          0\n",
       "26    04a0773c71fb          2\n",
       "27    04e1b77ef107          2\n",
       "28    051d9d12a6ee          2\n",
       "29    052e00f47cfa          2\n",
       "...            ...        ...\n",
       "1898  fc66648f758e          0\n",
       "1899  fc67df1e574e          2\n",
       "1900  fc6c9d0efe53          2\n",
       "1901  fc79feb5deed          0\n",
       "1902  fcbc1f4b5342          2\n",
       "1903  fcd166e6e4b5          2\n",
       "1904  fcea00df9f33          0\n",
       "1905  fd2978398705          2\n",
       "1906  fd2d58c6cd45          2\n",
       "1907  fd4d81b43e84          0\n",
       "1908  fd7cc592106e          2\n",
       "1909  fd8e6b0b2e45          0\n",
       "1910  fda8612fcc8c          0\n",
       "1911  fdde61dd1bde          3\n",
       "1912  fde8778182af          2\n",
       "1913  fe0a340c4477          2\n",
       "1914  fe190d618acf          2\n",
       "1915  fe1d2f703efc          2\n",
       "1916  fe5618ad2460          2\n",
       "1917  fe57ff56618e          0\n",
       "1918  fe84ad1df04b          2\n",
       "1919  fe920e47b72d          2\n",
       "1920  fed9d587f158          2\n",
       "1921  fee5bd042c3b          2\n",
       "1922  fef8e645d030          2\n",
       "1923  ff2fd94448de          0\n",
       "1924  ff4c945d9b17          2\n",
       "1925  ff64897ac0d8          2\n",
       "1926  ffa73465b705          3\n",
       "1927  ffdc2152d455          0\n",
       "\n",
       "[1928 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache cleaning\n",
    "if os.path.exists(\"cache\"):\n",
    "    for e in os.listdir(\"cache\"):\n",
    "        os.remove(os.path.join(\"cache\", e))\n",
    "    os.rmdir(\"cache\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
