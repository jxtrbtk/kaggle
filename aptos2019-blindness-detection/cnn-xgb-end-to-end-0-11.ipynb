{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "![CNN Schema](https://raw.githubusercontent.com/jxtrbtk/kaggle/master/aptos2019-blindness-detection/CNNSchema.png)\n",
    "\n",
    "This kernel is a playground around the following idea. CNN are made of two part, convultions that acts like a features extractor and a fully connected neural network used for classification. What if we replace the neural network classifier by an XGBoost based one ? XGBoost is most often very efficient and makes it easy to achieve very good performances without a lot of tuning effort.\n",
    "\n",
    "At the very beginning, I had better results with XGB, but the more optimized the process, the more similar the performance were...\n",
    "Hope you'll enjoy the journey !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports, settings and references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import time\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import quantile_transform\n",
    "\n",
    "import pickle\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\")\n",
    "DATA_SOURCE = os.path.join(\"..\",\"input\",\"aptos2019-blindness-detection\")\n",
    "MODEL_SOURCE = os.path.join(\"..\",\"input\",\"torchvisionmodelspartial1\")\n",
    "MODEL_SIZE = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(img,tol=7):\n",
    "    w, h = img.shape[1],img.shape[0]\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    gray_img = cv2.blur(gray_img,(5,5))\n",
    "    shape = gray_img.shape \n",
    "    gray_img = gray_img.reshape(-1,1)\n",
    "    quant = quantile_transform(gray_img, n_quantiles=256, random_state=0, copy=True)\n",
    "    quant = (quant*256).astype(int)\n",
    "    gray_img = quant.reshape(shape)\n",
    "    xp = (gray_img.mean(axis=0)>tol)\n",
    "    yp = (gray_img.mean(axis=1)>tol)\n",
    "    x1, x2 = np.argmax(xp), w-np.argmax(np.flip(xp))\n",
    "    y1, y2 = np.argmax(yp), h-np.argmax(np.flip(yp))\n",
    "    if x1 >= x2 or y1 >= y2 : # something wrong with the crop\n",
    "        return img # return original image\n",
    "    else:\n",
    "        img1=img[y1:y2,x1:x2,0]\n",
    "        img2=img[y1:y2,x1:x2,1]\n",
    "        img3=img[y1:y2,x1:x2,2]\n",
    "        img = np.stack([img1,img2,img3],axis=-1)\n",
    "    return img\n",
    "\n",
    "def process_image(image, size=512):\n",
    "    image = cv2.resize(image, (size,int(size*image.shape[0]/image.shape[1])))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    try:\n",
    "        image = crop_image(image, tol=15)\n",
    "    except Exception as e:\n",
    "        image = image\n",
    "        print( str(e) )\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*inspired by: https://www.kaggle.com/ratthachat/aptos-updated-preprocessing-ben-s-cropping*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch's style data loader defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "class RetinopathyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, transform, is_test=False):\n",
    "        self.transform = transform\n",
    "        self.base_transform = transforms.Resize((MODEL_SIZE, MODEL_SIZE))\n",
    "        self.is_test = is_test \n",
    "        if not os.path.exists(\"cache\"): os.mkdir(\"cache\")\n",
    "        if is_test : file = \"test.csv\"\n",
    "        else : file = \"train.csv\"\n",
    "        csv_file = os.path.join(DATA_SOURCE, file)\n",
    "        df = pd.read_csv(csv_file)\n",
    "        self.data = df.reset_index(drop=True)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_test : archive = \"test_images\"\n",
    "        else : archive = \"train_images\"\n",
    "        folder = os.path.join(DATA_SOURCE, archive)\n",
    "        code = str(self.data.loc[idx, 'id_code'])\n",
    "        file = code + \".png\"\n",
    "        cache_path = os.path.join(\"cache\",code+\".png\")\n",
    "        cached = os.path.exists(cache_path)\n",
    "        if not cached : \n",
    "            path = os.path.join(folder, file)\n",
    "            image = cv2.imread(path)\n",
    "            image = process_image(image)\n",
    "            imgpil = Image.fromarray(image)\n",
    "            imgpil = self.base_transform(imgpil)\n",
    "            imgpil.save(cache_path,\"PNG\")\n",
    "        imgpil = Image.open(cache_path)\n",
    "        img_tensor = self.transform(imgpil)\n",
    "        if self.is_test : return {'image': img_tensor} \n",
    "        else : \n",
    "            label = self.data.loc[idx, \"diagnosis\"]\n",
    "            return {'image': img_tensor, 'label': label}\n",
    "        \n",
    "\n",
    "    def get_df(self):\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*adapted from: https://www.kaggle.com/abhishek/very-simple-pytorch-training-0-59*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-train the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we will prepare the dataset and create the folds\n",
    "NUM_FOLDS = 5\n",
    "\n",
    "data_augmentation = transforms.Compose([\n",
    "    transforms.RandomRotation((-15, 15)),\n",
    "    transforms.Resize(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "DATA = RetinopathyDataset(data_augmentation)\n",
    "df = DATA.get_df()\n",
    "skf = StratifiedKFold(n_splits=NUM_FOLDS)\n",
    "folds_generator = skf.split(df.index.values, df.diagnosis.values)\n",
    "data_train, data_eval = [], [] \n",
    "for t, e in folds_generator:\n",
    "    data_train.append(t)\n",
    "    data_eval.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader_for_fold(n, data, train_data, eval_data, batch_size):\n",
    "    \"\"\" return the train and eval dataloader for a fold\n",
    "    \"\"\"    \n",
    "    train_sampler = SubsetRandomSampler(train_data[n])\n",
    "    valid_sampler = SubsetRandomSampler(eval_data[n])\n",
    "\n",
    "    data_loader_train = torch.utils.data.DataLoader(data, \n",
    "                    batch_size=batch_size, drop_last=False, \n",
    "                    sampler=train_sampler)\n",
    "    data_loader_eval = torch.utils.data.DataLoader(data, \n",
    "                    batch_size=batch_size, drop_last=False, \n",
    "                    sampler=valid_sampler)\n",
    "    \n",
    "    return data_loader_train, data_loader_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classificator(nn.Module):\n",
    "    \"\"\" classifier layer used to retrain the CNN\n",
    "    \"\"\"    \n",
    "    def __init__(self, size=128):\n",
    "        super(Classificator, self).__init__()\n",
    "        self.size = size\n",
    "        self.network = nn.Sequential(\n",
    "              nn.BatchNorm1d(size),\n",
    "              nn.Dropout(p=0.3),\n",
    "              nn.Linear(in_features=size, out_features=5, bias=True),\n",
    "        )        \n",
    "    def forward(self, x):\n",
    "        ## Define forward behavior\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_model():\n",
    "    \"\"\" get the pretrained model\n",
    "    \"\"\"    \n",
    "    model = torchvision.models.densenet161(pretrained=False)\n",
    "    model_path = os.path.join(MODEL_SOURCE, \"densenet161.pth\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    in_features = model.classifier.in_features\n",
    "    model.classifier = Classificator(in_features)\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, scheduler, train_data_loader, eval_data_loader, \n",
    "                file_name, num_epochs = 50, patience = 7, prev_loss = 1000.00):\n",
    "    \"\"\" train the model\n",
    "    arguments : model, optimizer, scheduler, train_data_loader, eval_data_loader\n",
    "        file_name: name of the file to save the best model \n",
    "        num_epochs: maximum number of epochs\n",
    "        patience: number of epochs to wait if no improvements\n",
    "        prev_loss: previous loss achieved, to surpass to have the model saved\n",
    "    return: \n",
    "        best loss achieved (previous loss if not surpassed)\n",
    "    \"\"\"    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    countdown = patience\n",
    "    best_loss = 1000.00\n",
    "    since = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        counter = 0\n",
    "        for bi, d in enumerate(train_data_loader):\n",
    "            inputs = d[\"image\"].to(DEVICE, dtype=torch.float)\n",
    "            labels = d[\"label\"].to(DEVICE, dtype=torch.long)\n",
    "            # batch norm layers needs more than 1 set of data\n",
    "            # this is to skip the last batch if it's only 1 image\n",
    "            if inputs.shape[0] > 1 :\n",
    "                counter += inputs.size(0)\n",
    "                model.to(DEVICE)\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs) \n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                loss_val = running_loss / counter\n",
    "                print(\"{:7} {:.4f} {:.4f}\".format(counter, loss.item()*1, loss_val), end=\"\\r\")\n",
    "        epoch_loss = running_loss / ( len(train_data_loader) * train_data_loader.batch_size)\n",
    "        time_elapsed = time.time() - since\n",
    "        print(\" T{:3}/{:3} loss: {:.4f} ({:3.0f}m {:2.0f}s)\".format( \n",
    "            epoch, num_epochs - 1, epoch_loss,time_elapsed // 60, time_elapsed % 60))\n",
    "        running_loss = 0.0\n",
    "        counter = 0\n",
    "        for bi, d in enumerate(eval_data_loader):\n",
    "            inputs = d[\"image\"].to(DEVICE, dtype=torch.float)\n",
    "            counter += inputs.size(0)\n",
    "            labels = d[\"label\"].to(DEVICE, dtype=torch.long)\n",
    "            model.to(DEVICE)\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            loss_val = running_loss / counter\n",
    "            print(\"{:7} {:.4f} {:.4f}\".format(counter, loss.item()*1, loss_val), end=\"\\r\")\n",
    "        epoch_loss = running_loss / ( len(eval_data_loader) * eval_data_loader.batch_size)\n",
    "        if epoch_loss < best_loss : \n",
    "            best_loss = epoch_loss\n",
    "            if epoch_loss < prev_loss:\n",
    "                torch.save(model.state_dict(), file_name)\n",
    "                prev_loss = epoch_loss\n",
    "                print(\"*\", end=\"\")\n",
    "            else:\n",
    "                print(\".\", end=\"\")\n",
    "            countdown = patience\n",
    "        else:\n",
    "            print(\"{:1}\".format(countdown), end=\"\")\n",
    "            countdown -= 1\n",
    "        time_elapsed = time.time() - since\n",
    "        print(\"E{:3}/{:3} loss: {:.4f} ({:3.0f}m {:2.0f}s)\".format( \n",
    "            epoch, num_epochs - 1, epoch_loss,time_elapsed // 60, time_elapsed % 60 ))\n",
    "        scheduler.step() #epoch_loss\n",
    "\n",
    "        if countdown <= 0 : break\n",
    "\n",
    "    return prev_loss\n",
    "    print(\"done.\")\n",
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- fold 0\n",
      "----------- round 0\n",
      " T  0/  8 loss: 0.6808 (  6m 43s)\n",
      "*E  0/  8 loss: 0.4871 (  8m 23s)\n",
      " T  1/  8 loss: 0.4522 (  9m 11s)\n",
      "*E  1/  8 loss: 0.4530 (  9m 17s)\n",
      " T  2/  8 loss: 0.4428 ( 10m  5s)\n",
      "*E  2/  8 loss: 0.4434 ( 10m 11s)\n",
      " T  3/  8 loss: 0.4064 ( 10m 60s)\n",
      "*E  3/  8 loss: 0.4359 ( 11m  6s)\n",
      " T  4/  8 loss: 0.3958 ( 11m 54s)\n",
      "*E  4/  8 loss: 0.4196 ( 11m 60s)\n",
      " T  5/  8 loss: 0.3731 ( 12m 48s)\n",
      "3E  5/  8 loss: 0.4291 ( 12m 54s)\n",
      " T  6/  8 loss: 0.3842 ( 13m 42s)\n",
      "2E  6/  8 loss: 0.4257 ( 13m 48s)\n",
      " T  7/  8 loss: 0.3816 ( 14m 37s)\n",
      "1E  7/  8 loss: 0.4265 ( 14m 42s)\n",
      "----------- round 1\n",
      " T  0/  8 loss: 0.6704 (  0m 49s)\n",
      ".E  0/  8 loss: 0.4782 (  0m 54s)\n",
      " T  1/  8 loss: 0.4402 (  1m 43s)\n",
      ".E  1/  8 loss: 0.4476 (  1m 48s)\n",
      " T  2/  8 loss: 0.4242 (  2m 37s)\n",
      ".E  2/  8 loss: 0.4337 (  2m 43s)\n",
      " T  3/  8 loss: 0.4137 (  3m 31s)\n",
      ".E  3/  8 loss: 0.4322 (  3m 37s)\n",
      " T  4/  8 loss: 0.3842 (  4m 25s)\n",
      "*E  4/  8 loss: 0.4191 (  4m 31s)\n",
      " T  5/  8 loss: 0.3813 (  5m 20s)\n",
      "3E  5/  8 loss: 0.4261 (  5m 25s)\n",
      " T  6/  8 loss: 0.3757 (  6m 14s)\n",
      "2E  6/  8 loss: 0.4225 (  6m 20s)\n",
      " T  7/  8 loss: 0.3833 (  7m  8s)\n",
      "1E  7/  8 loss: 0.4273 (  7m 14s)\n",
      "----------- round 2\n",
      " T  0/  8 loss: 0.6816 (  0m 49s)\n",
      ".E  0/  8 loss: 0.4892 (  0m 55s)\n",
      " T  1/  8 loss: 0.4527 (  1m 44s)\n",
      ".E  1/  8 loss: 0.4641 (  1m 50s)\n",
      " T  2/  8 loss: 0.4342 (  2m 39s)\n",
      ".E  2/  8 loss: 0.4495 (  2m 45s)\n",
      " T  3/  8 loss: 0.4101 (  3m 34s)\n",
      "3E  3/  8 loss: 0.4513 (  3m 39s)\n",
      " T  4/  8 loss: 0.4047 (  4m 28s)\n",
      "2E  4/  8 loss: 0.4545 (  4m 34s)\n",
      " T  5/  8 loss: 0.3910 (  5m 23s)\n",
      ".E  5/  8 loss: 0.4276 (  5m 29s)\n",
      " T  6/  8 loss: 0.3794 (  6m 18s)\n",
      "3E  6/  8 loss: 0.4336 (  6m 24s)\n",
      " T  7/  8 loss: 0.3760 (  7m 13s)\n",
      "2E  7/  8 loss: 0.4309 (  7m 18s)\n",
      " T  8/  8 loss: 0.3766 (  8m  7s)\n",
      "1E  8/  8 loss: 0.4285 (  8m 13s)\n",
      "---------------------- best loss 0.41912555721189293\n",
      "\n",
      "---------------------- fold 1\n",
      "----------- round 0\n",
      " T  0/  8 loss: 0.7037 (  0m 49s)\n",
      "*E  0/  8 loss: 0.4446 (  0m 55s)\n",
      " T  1/  8 loss: 0.4637 (  1m 44s)\n",
      "*E  1/  8 loss: 0.4231 (  1m 50s)\n",
      " T  2/  8 loss: 0.4329 (  2m 39s)\n",
      "3E  2/  8 loss: 0.4276 (  2m 45s)\n",
      " T  3/  8 loss: 0.4195 (  3m 34s)\n",
      "*E  3/  8 loss: 0.4115 (  3m 40s)\n",
      " T  4/  8 loss: 0.4010 (  4m 29s)\n",
      "3E  4/  8 loss: 0.4129 (  4m 34s)\n",
      " T  5/  8 loss: 0.3910 (  5m 23s)\n",
      "*E  5/  8 loss: 0.4060 (  5m 29s)\n",
      " T  6/  8 loss: 0.3918 (  6m 18s)\n",
      "3E  6/  8 loss: 0.4209 (  6m 24s)\n",
      " T  7/  8 loss: 0.3858 (  7m 13s)\n",
      "2E  7/  8 loss: 0.4111 (  7m 19s)\n",
      " T  8/  8 loss: 0.3811 (  8m  8s)\n",
      "1E  8/  8 loss: 0.4183 (  8m 14s)\n",
      "----------- round 1\n",
      " T  0/  8 loss: 0.6817 (  0m 49s)\n",
      ".E  0/  8 loss: 0.4712 (  0m 55s)\n",
      " T  1/  8 loss: 0.4534 (  1m 44s)\n",
      ".E  1/  8 loss: 0.4547 (  1m 49s)\n",
      " T  2/  8 loss: 0.4281 (  2m 38s)\n",
      ".E  2/  8 loss: 0.4297 (  2m 44s)\n",
      " T  3/  8 loss: 0.4136 (  3m 33s)\n",
      ".E  3/  8 loss: 0.4159 (  3m 39s)\n",
      " T  4/  8 loss: 0.3923 (  4m 28s)\n",
      ".E  4/  8 loss: 0.4103 (  4m 34s)\n",
      " T  5/  8 loss: 0.3836 (  5m 23s)\n",
      "3E  5/  8 loss: 0.4306 (  5m 28s)\n",
      " T  6/  8 loss: 0.3892 (  6m 17s)\n",
      "2E  6/  8 loss: 0.4105 (  6m 23s)\n",
      " T  7/  8 loss: 0.3866 (  7m 12s)\n",
      "1E  7/  8 loss: 0.4132 (  7m 18s)\n",
      "----------- round 2\n",
      " T  0/  8 loss: 0.7056 (  0m 49s)\n",
      ".E  0/  8 loss: 0.4426 (  0m 55s)\n",
      " T  1/  8 loss: 0.4563 (  1m 44s)\n",
      ".E  1/  8 loss: 0.4309 (  1m 50s)\n",
      " T  2/  8 loss: 0.4309 (  2m 39s)\n",
      ".E  2/  8 loss: 0.4138 (  2m 44s)\n",
      " T  3/  8 loss: 0.4071 (  3m 33s)\n",
      ".E  3/  8 loss: 0.4123 (  3m 39s)\n",
      " T  4/  8 loss: 0.3997 (  4m 28s)\n",
      "*E  4/  8 loss: 0.3941 (  4m 34s)\n",
      " T  5/  8 loss: 0.3820 (  5m 23s)\n",
      "3E  5/  8 loss: 0.4101 (  5m 29s)\n",
      " T  6/  8 loss: 0.3764 (  6m 18s)\n",
      "2E  6/  8 loss: 0.3957 (  6m 23s)\n",
      " T  7/  8 loss: 0.3794 (  7m 12s)\n",
      "1E  7/  8 loss: 0.4021 (  7m 18s)\n",
      "---------------------- best loss 0.3941114468081873\n",
      "\n",
      "---------------------- fold 2\n",
      "----------- round 0\n",
      " T  0/  8 loss: 0.7073 (  0m 49s)\n",
      "*E  0/  8 loss: 0.4724 (  0m 55s)\n",
      " T  1/  8 loss: 0.4660 (  1m 44s)\n",
      "*E  1/  8 loss: 0.4340 (  1m 50s)\n",
      " T  2/  8 loss: 0.4450 (  2m 39s)\n",
      "*E  2/  8 loss: 0.4214 (  2m 45s)\n",
      " T  3/  8 loss: 0.4228 (  3m 33s)\n",
      "*E  3/  8 loss: 0.4129 (  3m 39s)\n",
      " T  4/  8 loss: 0.4053 (  4m 28s)\n",
      "*E  4/  8 loss: 0.4114 (  4m 34s)\n",
      " T  5/  8 loss: 0.4049 (  5m 23s)\n",
      "3E  5/  8 loss: 0.4123 (  5m 29s)\n",
      " T  6/  8 loss: 0.3833 (  6m 18s)\n",
      "2E  6/  8 loss: 0.4156 (  6m 24s)\n",
      " T  7/  8 loss: 0.3894 (  7m 13s)\n",
      "*E  7/  8 loss: 0.4029 (  7m 18s)\n",
      " T  8/  8 loss: 0.3830 (  8m  7s)\n",
      "3E  8/  8 loss: 0.4129 (  8m 13s)\n",
      "----------- round 1\n",
      " T  0/  8 loss: 0.6858 (  0m 49s)\n",
      ".E  0/  8 loss: 0.4784 (  0m 55s)\n",
      " T  1/  8 loss: 0.4627 (  1m 44s)\n",
      ".E  1/  8 loss: 0.4267 (  1m 49s)\n",
      " T  2/  8 loss: 0.4263 (  2m 38s)\n",
      ".E  2/  8 loss: 0.4213 (  2m 44s)\n",
      " T  3/  8 loss: 0.4081 (  3m 33s)\n",
      ".E  3/  8 loss: 0.4089 (  3m 39s)\n",
      " T  4/  8 loss: 0.4058 (  4m 27s)\n",
      ".E  4/  8 loss: 0.4038 (  4m 33s)\n",
      " T  5/  8 loss: 0.3991 (  5m 22s)\n",
      ".E  5/  8 loss: 0.4030 (  5m 28s)\n",
      " T  6/  8 loss: 0.3752 (  6m 17s)\n",
      "*E  6/  8 loss: 0.3992 (  6m 23s)\n",
      " T  7/  8 loss: 0.3920 (  7m 12s)\n",
      "3E  7/  8 loss: 0.4085 (  7m 18s)\n",
      " T  8/  8 loss: 0.3749 (  8m  7s)\n",
      "*E  8/  8 loss: 0.3975 (  8m 13s)\n",
      "----------- round 2\n",
      " T  0/  8 loss: 0.6985 (  0m 49s)\n",
      ".E  0/  8 loss: 0.4470 (  0m 55s)\n",
      " T  1/  8 loss: 0.4724 (  1m 44s)\n",
      ".E  1/  8 loss: 0.4384 (  1m 49s)\n",
      " T  2/  8 loss: 0.4360 (  2m 38s)\n",
      ".E  2/  8 loss: 0.4134 (  2m 44s)\n",
      " T  3/  8 loss: 0.4289 (  3m 33s)\n",
      "3E  3/  8 loss: 0.4290 (  3m 39s)\n",
      " T  4/  8 loss: 0.3992 (  4m 28s)\n",
      ".E  4/  8 loss: 0.4073 (  4m 34s)\n",
      " T  5/  8 loss: 0.3844 (  5m 23s)\n",
      ".E  5/  8 loss: 0.4033 (  5m 28s)\n",
      " T  6/  8 loss: 0.3832 (  6m 17s)\n",
      "*E  6/  8 loss: 0.3965 (  6m 23s)\n",
      " T  7/  8 loss: 0.3845 (  7m 12s)\n",
      "3E  7/  8 loss: 0.4018 (  7m 18s)\n",
      " T  8/  8 loss: 0.3889 (  8m  7s)\n",
      "2E  8/  8 loss: 0.4159 (  8m 12s)\n",
      "---------------------- best loss 0.39650120206143974\n",
      "\n",
      "---------------------- fold 3\n",
      "----------- round 0\n",
      " T  0/  8 loss: 0.6846 (  0m 49s)\n",
      "*E  0/  8 loss: 0.4788 (  0m 55s)\n",
      " T  1/  8 loss: 0.4568 (  1m 44s)\n",
      "*E  1/  8 loss: 0.4355 (  1m 50s)\n",
      " T  2/  8 loss: 0.4198 (  2m 39s)\n",
      "*E  2/  8 loss: 0.4297 (  2m 45s)\n",
      " T  3/  8 loss: 0.4084 (  3m 34s)\n",
      "*E  3/  8 loss: 0.4206 (  3m 40s)\n",
      " T  4/  8 loss: 0.3872 (  4m 29s)\n",
      "*E  4/  8 loss: 0.4150 (  4m 34s)\n",
      " T  5/  8 loss: 0.3878 (  5m 24s)\n",
      "3E  5/  8 loss: 0.4163 (  5m 29s)\n",
      " T  6/  8 loss: 0.3802 (  6m 18s)\n",
      "*E  6/  8 loss: 0.4110 (  6m 24s)\n",
      " T  7/  8 loss: 0.3814 (  7m 13s)\n",
      "*E  7/  8 loss: 0.4046 (  7m 19s)\n",
      " T  8/  8 loss: 0.3668 (  8m  8s)\n",
      "3E  8/  8 loss: 0.4151 (  8m 14s)\n",
      "----------- round 1\n",
      " T  0/  8 loss: 0.7000 (  0m 49s)\n",
      ".E  0/  8 loss: 0.4942 (  0m 55s)\n",
      " T  1/  8 loss: 0.4690 (  1m 44s)\n",
      ".E  1/  8 loss: 0.4409 (  1m 49s)\n",
      " T  2/  8 loss: 0.4426 (  2m 38s)\n",
      ".E  2/  8 loss: 0.4277 (  2m 44s)\n",
      " T  3/  8 loss: 0.4201 (  3m 33s)\n",
      ".E  3/  8 loss: 0.4228 (  3m 39s)\n",
      " T  4/  8 loss: 0.4072 (  4m 28s)\n",
      ".E  4/  8 loss: 0.4152 (  4m 33s)\n",
      " T  5/  8 loss: 0.3913 (  5m 22s)\n",
      "3E  5/  8 loss: 0.4172 (  5m 28s)\n",
      " T  6/  8 loss: 0.3893 (  6m 17s)\n",
      "2E  6/  8 loss: 0.4214 (  6m 23s)\n",
      " T  7/  8 loss: 0.3906 (  7m 12s)\n",
      ".E  7/  8 loss: 0.4145 (  7m 18s)\n",
      " T  8/  8 loss: 0.3937 (  8m  7s)\n",
      "3E  8/  8 loss: 0.4220 (  8m 12s)\n",
      "----------- round 2\n",
      " T  0/  8 loss: 0.7036 (  0m 49s)\n",
      ".E  0/  8 loss: 0.4827 (  0m 55s)\n",
      " T  1/  8 loss: 0.4729 (  1m 44s)\n",
      ".E  1/  8 loss: 0.4409 (  1m 49s)\n",
      " T  2/  8 loss: 0.4382 (  2m 38s)\n",
      "3E  2/  8 loss: 0.4509 (  2m 44s)\n",
      " T  3/  8 loss: 0.4321 (  3m 33s)\n",
      ".E  3/  8 loss: 0.4304 (  3m 39s)\n",
      " T  4/  8 loss: 0.4082 (  4m 28s)\n",
      ".E  4/  8 loss: 0.4205 (  4m 34s)\n",
      " T  5/  8 loss: 0.3893 (  5m 23s)\n",
      ".E  5/  8 loss: 0.4112 (  5m 28s)\n",
      " T  6/  8 loss: 0.3808 (  6m 17s)\n",
      "3E  6/  8 loss: 0.4363 (  6m 23s)\n",
      " T  7/  8 loss: 0.3810 (  7m 12s)\n",
      "2E  7/  8 loss: 0.4200 (  7m 18s)\n",
      " T  8/  8 loss: 0.3840 (  8m  7s)\n",
      "1E  8/  8 loss: 0.4168 (  8m 13s)\n",
      "---------------------- best loss 0.40458750177402886\n",
      "\n",
      "---------------------- fold 4\n",
      "----------- round 0\n",
      " T  0/  8 loss: 0.6853 (  0m 49s)\n",
      "*E  0/  8 loss: 0.5379 (  0m 55s)\n",
      " T  1/  8 loss: 0.4396 (  1m 44s)\n",
      "*E  1/  8 loss: 0.4847 (  1m 50s)\n",
      " T  2/  8 loss: 0.4193 (  2m 39s)\n",
      "*E  2/  8 loss: 0.4767 (  2m 45s)\n",
      " T  3/  8 loss: 0.3979 (  3m 34s)\n",
      "3E  3/  8 loss: 0.4814 (  3m 40s)\n",
      " T  4/  8 loss: 0.3854 (  4m 29s)\n",
      "*E  4/  8 loss: 0.4767 (  4m 35s)\n",
      " T  5/  8 loss: 0.3695 (  5m 24s)\n",
      "*E  5/  8 loss: 0.4698 (  5m 30s)\n",
      " T  6/  8 loss: 0.3692 (  6m 19s)\n",
      "3E  6/  8 loss: 0.4921 (  6m 25s)\n",
      " T  7/  8 loss: 0.3667 (  7m 14s)\n",
      "2E  7/  8 loss: 0.4713 (  7m 20s)\n",
      "   2576 0.3312 0.3684\r"
     ]
    }
   ],
   "source": [
    "# train the model num_round_per_fold times for each fold\n",
    "# and the save the best model for each fold\n",
    "batch_size = 56\n",
    "num_round_per_fold = 3\n",
    "for no in range(NUM_FOLDS):\n",
    "    print(\"-\"*22, \"fold\",no)\n",
    "    bst_loss = 10000.00\n",
    "    for r in range(num_round_per_fold):\n",
    "        print(\"-\"*11,\"round\",r)\n",
    "        data_loader_train, data_loader_eval = get_dataloader_for_fold(no, \n",
    "                                    DATA, data_train, data_eval, batch_size)\n",
    "        model = get_base_model()\n",
    "        plist = [{\"params\": model.features.denseblock3.parameters(), \"lr\":0.0001},\n",
    "                 {\"params\": model.features.denseblock4.parameters(), \"lr\":0.0001},\n",
    "                 {\"params\": model.classifier.parameters()}]\n",
    "        optimizer = optim.Adam(plist, lr=0.001, amsgrad=True)\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [1,5], gamma=0.1, last_epoch=-1)        \n",
    "        bst_loss = train_model(model, optimizer, scheduler, \n",
    "                               data_loader_train, data_loader_eval, \n",
    "                               \"tmp\"+str(no)+\".pth\", prev_loss=bst_loss, \n",
    "                               num_epochs=9, patience=3)\n",
    "    print(\"-\"*22, \"best loss\", bst_loss)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract train features from CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trained_model(no): \n",
    "    \"\"\" reload and return the retrained model for the given fold \n",
    "    \"\"\"    \n",
    "    extractor = torchvision.models.densenet161(pretrained=False)\n",
    "    in_features = extractor.classifier.in_features\n",
    "    extractor.classifier = Classificator(in_features)\n",
    "    model_path = os.path.join(\"tmp\"+str(no)+\".pth\") #no\n",
    "    extractor.load_state_dict(torch.load(model_path))\n",
    "    extractor = extractor.to(DEVICE)\n",
    "    extractor.eval()\n",
    "    return extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extractor_model(no):\n",
    "    \"\"\" reload and return the retrained model for the given fold\n",
    "        and make last layer identity\n",
    "    \"\"\"    \n",
    "    extractor = get_trained_model(no)\n",
    "    extractor.classifier = nn.Identity()\n",
    "    extractor = extractor.to(DEVICE)\n",
    "    extractor.eval()\n",
    "    return extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_features(data_loader, extractor):\n",
    "    \"\"\" return 2 arrays of features extracted, and targets  \n",
    "    \"\"\"    \n",
    "    for bi, d in enumerate(data_loader):\n",
    "        print(\".\", end=\"\")\n",
    "        img_tensor = d[\"image\"].to(DEVICE)\n",
    "        target = d[\"label\"].numpy()\n",
    "        with torch.no_grad(): feature = extractor(img_tensor)\n",
    "        feature = feature.cpu().detach().squeeze(0).numpy()\n",
    "        if bi == 0 :\n",
    "            features = feature \n",
    "            targets = target \n",
    "        else :\n",
    "            features = np.concatenate([features, feature], axis=0)\n",
    "            targets = np.concatenate([targets, target], axis=0)\n",
    "\n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBOOST_PARAM = {\n",
    "    \"random_state\"      : 42,\n",
    "    \"n_estimators\"      : 200,\n",
    "    \"objective\"         : \"multi:softmax\",\n",
    "    \"num_class\"         : 5,\n",
    "    \"eval_metric\"       : \"mlogloss\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- fold 0\n",
      "...........|.............................................|\n",
      "..........................................................\n",
      "score 0.484875\n",
      "---------------------- fold 1\n",
      "...........|.............................................|\n",
      "..........................................................\n",
      "score 0.480186\n",
      "---------------------- fold 2\n",
      "...........|.............................................|\n",
      "..........................................................\n",
      "score 0.468014\n",
      "---------------------- fold 3\n",
      "...........|.............................................|\n",
      "..........................................................\n",
      "score 0.452169\n",
      "---------------------- fold 4\n",
      "...........|.............................................|\n",
      "..........................................................\n",
      "score 0.542255\n"
     ]
    }
   ],
   "source": [
    "# for each fold, get the data loader, extractor, \n",
    "# extract the features (loaded and process each time, but we can have data aug.) \n",
    "# calcul the weights table, create and fit the XGB model\n",
    "batch_size = 64\n",
    "eval_set = []\n",
    "for no in range(NUM_FOLDS):\n",
    "    print(\"-\"*22, \"fold\",no)\n",
    "    data_loader_train, data_loader_eval = get_dataloader_for_fold(no, \n",
    "                                DATA, data_train, data_eval, batch_size)\n",
    "    extractor = get_extractor_model(no)\n",
    "\n",
    "    print(\"...........|.............................................|\")\n",
    "    features_eval, targets_eval = get_train_features(data_loader_eval,\n",
    "                                                     extractor)\n",
    "    features_train, targets_train = get_train_features(data_loader_train,\n",
    "                                                       extractor)\n",
    "    print(\"\")\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(**XGBOOST_PARAM)\n",
    "    xgb_model = xgb_model.fit(features_train,targets_train.reshape(-1),\n",
    "                        eval_set=[(features_eval, targets_eval.reshape(-1))],\n",
    "                        early_stopping_rounds=20,\n",
    "                        verbose=False)\n",
    "    print(\"score\",xgb_model.evals_result()[\"validation_0\"][\"mlogloss\"][-1])\n",
    "    pickle.dump(xgb_model, open(\"xgb_model_\"+str(no), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "## .using the full CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the data augmentation of the dataset object\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "DATA.transform = base_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen Kappa quadratic score 0.8869850023318407\n"
     ]
    }
   ],
   "source": [
    "# first we make predictions using the CNN models for each fold \n",
    "# and calculate the score of all predictions\n",
    "predictions, targets = np.zeros(len(DATA)), np.zeros(len(DATA))\n",
    "batch_slice = (0, 0)\n",
    "softmax = nn.Softmax(dim=1)\n",
    "for no in range(NUM_FOLDS):\n",
    "    _, data_loader_eval = get_dataloader_for_fold(no, \n",
    "                DATA, data_train, data_eval, 64)\n",
    "    model = get_trained_model(no) #no\n",
    "    for bi, d in enumerate(data_loader_eval):\n",
    "        inputs = d[\"image\"].to(DEVICE, dtype=torch.float)\n",
    "        batch_slice = (batch_slice[1], batch_slice[1]+inputs.size(0))\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            outputs = softmax(outputs)\n",
    "        predictions[batch_slice[0]:batch_slice[1]] = \\\n",
    "                outputs.cpu().detach().squeeze(0).numpy().argmax(axis=1)\n",
    "        targets[batch_slice[0]:batch_slice[1]] = d[\"label\"]\n",
    "        \n",
    "print(\"Cohen Kappa quadratic score\", \n",
    "      cohen_kappa_score(targets, predictions, weights=\"quadratic\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .using the XGB models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............\n",
      "............\n",
      "............\n",
      "............\n",
      "............\n",
      "Cohen Kappa quadratic score 0.8174876272756739\n"
     ]
    }
   ],
   "source": [
    "# now let's do it with the XGB models \n",
    "predictions, targets = np.zeros(0), np.zeros(0)\n",
    "for no in range(NUM_FOLDS):\n",
    "    _, data_loader_eval = get_dataloader_for_fold(no, \n",
    "                                DATA, data_train, data_eval, batch_size)\n",
    "    features_eval, targets_eval = get_train_features(data_loader_eval,\n",
    "                                                     extractor)\n",
    "    print(\"\")\n",
    "    xgb_model = xgb.XGBClassifier()\n",
    "    model_path = os.path.join(\"xgb_model_\"+str(no))\n",
    "    xgb_model = pickle.load(open(model_path, \"rb\"))\n",
    "    prediction = xgb_model.predict(features_eval)\n",
    "    predictions = np.concatenate([predictions, prediction], axis=0)\n",
    "    targets = np.concatenate([targets, targets_eval], axis=0)\n",
    "\n",
    "print(\"Cohen Kappa quadratic score\", \n",
    "      cohen_kappa_score(targets, predictions, weights=\"quadratic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning\n",
    "if os.path.exists(\"cache\"):\n",
    "    for e in os.listdir(\"cache\"):\n",
    "        os.remove(os.path.join(\"cache\", e))\n",
    "    os.rmdir(\"cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract test features using CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = transforms.Compose([\n",
    "    transforms.Resize((MODEL_SIZE, MODEL_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "data_test = RetinopathyDataset(data_augmentation, is_test=True)\n",
    "data_loader = torch.utils.data.DataLoader(data_test, \n",
    "                            batch_size=16, shuffle=False, \n",
    "                            num_workers=0, drop_last=False)\n",
    "\n",
    "def get_test_features(data_loader, extractor):\n",
    "    \"\"\" return an array of features extracted  \n",
    "    \"\"\"    \n",
    "    for bi, d in enumerate(data_loader):\n",
    "        if bi % 8 == 0 : print(\".\", end=\"\")\n",
    "        img_tensor = d[\"image\"].to(DEVICE)\n",
    "        with torch.no_grad(): feature = extractor(img_tensor)\n",
    "        feature = feature.cpu().detach().numpy() #.squeeze(0) for batch_size > 1\n",
    "        if bi == 0 :\n",
    "            features = feature \n",
    "        else :\n",
    "            features = np.concatenate([features, feature], axis=0)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction using XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................ v\n",
      "............tta0................\n",
      "................ 0\n",
      "................ 1\n",
      "................ 2\n",
      "................ 3\n",
      "................ 4\n",
      "............tta1................\n",
      "................ 0\n",
      "................ 1\n",
      "................ 2\n",
      "................ 3\n",
      "................ 4\n"
     ]
    }
   ],
   "source": [
    "# adding prediction for each model \n",
    "# we can loop several time to perform data augmentation (tta) \n",
    "# (note: a bit risky as we cache the image, should be done bucket by bucket\n",
    "#  and clean after each bucket to avoid filling all the disk space)\n",
    "print(\"................................ v\")\n",
    "predictions = np.zeros((len(data_test),5))\n",
    "for tta in range(2):\n",
    "    print(\"............tta\"+str(tta)+\"................\")\n",
    "    for no in range(NUM_FOLDS):\n",
    "        extractor = get_extractor_model(no)\n",
    "        features = get_test_features(data_loader, extractor)\n",
    "        print(\"\",no)\n",
    "        xgb_model = xgb.XGBClassifier()\n",
    "        model_path = os.path.join(\"xgb_model_\"+str(no))\n",
    "        xgb_model = pickle.load(open(model_path, \"rb\"))\n",
    "        prediction = xgb_model.predict_proba(features)\n",
    "        predictions = predictions + prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voting\n",
    "prediction_final = predictions.argmax(axis=1)\n",
    "csv_file = os.path.join(DATA_SOURCE, \"sample_submission.csv\")\n",
    "df = pd.read_csv(csv_file)\n",
    "df[\"diagnosis\"] = prediction_final\n",
    "df.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0005cfc8afb6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003f0afdcd15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006efc72b638</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00836aaacf06</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>009245722fa4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>009c019a7309</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>010d915e229a</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0111b949947e</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>01499815e469</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0167076e7089</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>01c31b10ab99</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>01c5ba195207</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>01e4d86b3a30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>020921b796d5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>020f6983114d</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>021c207614d6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0229c0a80d42</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>024d0a225db1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0268f4382c67</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0299d97f31f7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>03042a663e54</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>030e06ddbb04</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>033cdbbbdfaa</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>03be80919be4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>03eaa4eef484</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0434995d0654</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>04a0773c71fb</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>04e1b77ef107</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>051d9d12a6ee</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>052e00f47cfa</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1898</th>\n",
       "      <td>fc66648f758e</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>fc67df1e574e</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>fc6c9d0efe53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901</th>\n",
       "      <td>fc79feb5deed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902</th>\n",
       "      <td>fcbc1f4b5342</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1903</th>\n",
       "      <td>fcd166e6e4b5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>fcea00df9f33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905</th>\n",
       "      <td>fd2978398705</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906</th>\n",
       "      <td>fd2d58c6cd45</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1907</th>\n",
       "      <td>fd4d81b43e84</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1908</th>\n",
       "      <td>fd7cc592106e</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909</th>\n",
       "      <td>fd8e6b0b2e45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1910</th>\n",
       "      <td>fda8612fcc8c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>fdde61dd1bde</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1912</th>\n",
       "      <td>fde8778182af</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913</th>\n",
       "      <td>fe0a340c4477</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1914</th>\n",
       "      <td>fe190d618acf</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>fe1d2f703efc</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>fe5618ad2460</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>fe57ff56618e</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>fe84ad1df04b</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>fe920e47b72d</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>fed9d587f158</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>fee5bd042c3b</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>fef8e645d030</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923</th>\n",
       "      <td>ff2fd94448de</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1924</th>\n",
       "      <td>ff4c945d9b17</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1925</th>\n",
       "      <td>ff64897ac0d8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1926</th>\n",
       "      <td>ffa73465b705</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927</th>\n",
       "      <td>ffdc2152d455</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1928 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id_code  diagnosis\n",
       "0     0005cfc8afb6          1\n",
       "1     003f0afdcd15          2\n",
       "2     006efc72b638          2\n",
       "3     00836aaacf06          2\n",
       "4     009245722fa4          2\n",
       "5     009c019a7309          2\n",
       "6     010d915e229a          3\n",
       "7     0111b949947e          0\n",
       "8     01499815e469          3\n",
       "9     0167076e7089          0\n",
       "10    01c31b10ab99          2\n",
       "11    01c5ba195207          2\n",
       "12    01e4d86b3a30          2\n",
       "13    020921b796d5          2\n",
       "14    020f6983114d          2\n",
       "15    021c207614d6          1\n",
       "16    0229c0a80d42          2\n",
       "17    024d0a225db1          2\n",
       "18    0268f4382c67          2\n",
       "19    0299d97f31f7          0\n",
       "20    03042a663e54          2\n",
       "21    030e06ddbb04          2\n",
       "22    033cdbbbdfaa          2\n",
       "23    03be80919be4          2\n",
       "24    03eaa4eef484          2\n",
       "25    0434995d0654          0\n",
       "26    04a0773c71fb          2\n",
       "27    04e1b77ef107          2\n",
       "28    051d9d12a6ee          2\n",
       "29    052e00f47cfa          2\n",
       "...            ...        ...\n",
       "1898  fc66648f758e          0\n",
       "1899  fc67df1e574e          2\n",
       "1900  fc6c9d0efe53          0\n",
       "1901  fc79feb5deed          0\n",
       "1902  fcbc1f4b5342          2\n",
       "1903  fcd166e6e4b5          2\n",
       "1904  fcea00df9f33          1\n",
       "1905  fd2978398705          2\n",
       "1906  fd2d58c6cd45          2\n",
       "1907  fd4d81b43e84          2\n",
       "1908  fd7cc592106e          2\n",
       "1909  fd8e6b0b2e45          0\n",
       "1910  fda8612fcc8c          0\n",
       "1911  fdde61dd1bde          3\n",
       "1912  fde8778182af          2\n",
       "1913  fe0a340c4477          2\n",
       "1914  fe190d618acf          2\n",
       "1915  fe1d2f703efc          2\n",
       "1916  fe5618ad2460          2\n",
       "1917  fe57ff56618e          0\n",
       "1918  fe84ad1df04b          2\n",
       "1919  fe920e47b72d          2\n",
       "1920  fed9d587f158          2\n",
       "1921  fee5bd042c3b          2\n",
       "1922  fef8e645d030          2\n",
       "1923  ff2fd94448de          0\n",
       "1924  ff4c945d9b17          2\n",
       "1925  ff64897ac0d8          2\n",
       "1926  ffa73465b705          4\n",
       "1927  ffdc2152d455          0\n",
       "\n",
       "[1928 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache cleaning\n",
    "if os.path.exists(\"cache\"):\n",
    "    for e in os.listdir(\"cache\"):\n",
    "        os.remove(os.path.join(\"cache\", e))\n",
    "    os.rmdir(\"cache\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
