{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = {\n",
    "    \"pre_Scaler\" : \"std\",           # (None, std, minmax)\n",
    "    \"pre_QuantiletTrans\" : True, \n",
    "    \"data_BatchSize\" : 125,\n",
    "    \"data_NbFolds\" : 10,\n",
    "    \"data_NbValidFolds\" : 2,\n",
    "    \"data_UpSampleCoeff\" : 0.10,    # add label 1 data x coeff\n",
    "    \"data_DownSampleCoeff\" : 0.95,  # multiply label 0 by coeff\n",
    "    \"mod_Act\" : \"leaky\",            # (relu, leaky, tanh, sig) tanh/no\n",
    "    \"mod_ActSlope\" : 0.02,          # for leaky\n",
    "    \"mod_NbLayers\" : 3, \n",
    "    \"mod_BatchNorm\" : \"after\",      # empty, before or after = after for relu ?!\n",
    "    \"mod_LayerCoeff\": 0.9, \n",
    "    \"mod_Dropout\": 0.45, \n",
    "    \"mod_HiddenSize\": 520,          # 520\n",
    "    \"mod_InputSize\": 200,           # 200+ for data augmentation\n",
    "    \"train_MaxLoop\":   7,           # \n",
    "    \"train_MaxEpoch\": 100,          # max epoch avant stop (1 pour test / 80 déjà atteint avec reg L1)\n",
    "    \"train_Loss\": \"bce\",            # bce or mse\n",
    "    \"train_CountDown\": 5,\n",
    "    \"train_AugLossAlpha\": 0.02,     #  \n",
    "    \"train_LbdL1\": 0.00001,         # 0.0001, 0.00005 0.0001 peut être OK, voir à la baisse \n",
    "    \"train_LbdL2\": 0.00005,          # converge : mse=0.0032, 0.0001 impact faible \n",
    "    \"train_LbdAUC\": 0.00010,         # 0.1000 stupid idea.... (should be with a big batch size for AUC calculation)\n",
    "    \"train_Opt\": \"SGD\",             # SGD or Adam\n",
    "    \"train_LearnRate\": 0.001,\n",
    "    \"train_SGDmomentum\": 0.92,      # for SGD : room for value between 0.9 and <1.0\n",
    "    \"valid_Metric\": \"auc\",       # \"L1loss\" oo \"auc\"\n",
    "    \"train_BoostingValid\":  50,     # Augment validation data where failed\n",
    "    \"train_BoostingTrain\":  50,     # Augment training data where failed\n",
    "}\n",
    "# note try boosting = upsmapling top error / balanced class / per fold...\n",
    "\n",
    "P_Boosting_Top_valid = P[\"train_BoostingValid\"]\n",
    "P_Boosting_Top_train = P[\"train_BoostingTrain\"]\n",
    "P_Boosting = False\n",
    "if P_Boosting_Top_valid > 0 or P_Boosting_Top_train > 0:\n",
    "    P_Boosting = True\n",
    "# P_Index = 43\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random, pickle \n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if P_Boosting == False:\n",
    "df_grid = pd.read_csv(\"grid.csv\", index_col=0)\n",
    "df_grid = df_grid.append(pd.Series(P), ignore_index=True)\n",
    "df_grid.to_csv(\"grid.csv\", index=True)\n",
    "grid_index = max(df_grid.index)\n",
    "P_Index = grid_index\n",
    "# else:\n",
    "#     grid_index = P_Index\n",
    "# print(grid_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('train.csv.zip', float_precision=\"float64\")\n",
    "df_test = pd.read_csv('test.csv.zip', float_precision=\"float64\")\n",
    "df = pd.concat([df_train, df_test], ignore_index=True, sort=True)\n",
    "\n",
    "df_X = df.drop(columns=[\"ID_code\", \"target\"])\n",
    "Y = df_train[\"target\"].values.astype('float64')\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    }
   ],
   "source": [
    "if P[\"pre_Scaler\"] == \"std\":\n",
    "    scaler = StandardScaler()\n",
    "    df_Xn = pd.DataFrame(scaler.fit_transform(df_X))\n",
    "if P[\"pre_Scaler\"] == \"minmax\":\n",
    "    scaler = MinMaxScaler()\n",
    "    df_Xn = pd.DataFrame(scaler.fit_transform(df_X))\n",
    "if P[\"pre_Scaler\"] == \"none\":\n",
    "    df_Xn = df_X.copy()\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    }
   ],
   "source": [
    "if P[\"pre_QuantiletTrans\"] : \n",
    "    df_Xt = pd.DataFrame(QuantileTransformer(output_distribution='normal').fit_transform(df_Xn))\n",
    "else:\n",
    "    df_Xt = df_Xn.copy()\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    }
   ],
   "source": [
    "idx = []\n",
    "idx = [i for i in range(200_000)]\n",
    "random.shuffle(idx)\n",
    "X_train = df_Xt.values.astype('float64')[:200_000] \n",
    "Y_train = df_train[\"target\"].values.astype('float64')\n",
    "X_train = [X_train[i] for i in idx] \n",
    "Y_train = [Y_train[i] for i in idx]\n",
    "X_train = np.array(X_train) \n",
    "Y_train = np.array(Y_train)\n",
    "Z_train = df_Xt.values.astype('float64')\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train,Y_train)\n",
    "Y_gnb = gnb.predict_proba(Z_train)[:,1]\n",
    "df_Xt[\"gnb\"] = Y_gnb\n",
    "gnb = None\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/vishalbajaj2000\n",
    "LGBGBDT_PARAM = {\n",
    "    'random_state' : 1981,\n",
    "    'n_estimators' : 2000,\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 16,\n",
    "    'max_depth': 4,\n",
    "    'metric' : ['mae'],\n",
    "    'boosting_type' : 'gbdt',\n",
    "    'objective' : 'binary',\n",
    "    'reg_alpha' : 2.03,\n",
    "    'reg_lambda' : 4.7,\n",
    "    'feature_fraction' : 0.8, #colsample_bytree\n",
    "    'feature_fraction_seed' : 1981, \n",
    "    'max_bins' : 100,\n",
    "    'min_split_gain': 0.0148,\n",
    "    'min_child_weight' : 7.835, #min_sum_hessian_in_leaf \n",
    "    'min_data_in_leaf' : 1000, #min_child_samples\n",
    "    'random_state' : 1981, # Updated from 'seed'\n",
    "    'subsample' : .912, #also known as Bagging fraction!\n",
    "    'subsample_freq' : 200, # also known as bagging frequency!\n",
    "    'boost_from_average' : False,\n",
    "    'verbose_eval' : 50,\n",
    "    'is_unbalance' : True,\n",
    "    #'scale_pos_weight' : 10.1,\n",
    "    }\n",
    "\n",
    "idx = []\n",
    "idx = [i for i in range(200_000)]\n",
    "random.shuffle(idx)\n",
    "X_train = [X_train[i] for i in idx] \n",
    "Y_train = [Y_train[i] for i in idx]\n",
    "X_train = np.array(X_train) \n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "LGBGBDT = lgb.LGBMClassifier( **LGBGBDT_PARAM, n_jobs=2, verbosity=0)\n",
    "LGBGBDT_FIT = LGBGBDT.fit(X_train[20_000:200_000],Y_train[20_000:200_000], \n",
    "                          eval_set=[(X_train[:20_000],Y_train[:20_000])], \n",
    "                          eval_metric= ['auc','binary_logloss'], early_stopping_rounds=100, verbose=False)\n",
    "Y_lgb = LGBGBDT.predict_proba(Z_train)[:,1]\n",
    "df_Xt[\"lgb\"] = Y_lgb\n",
    "\n",
    "LGBGBDT = None\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:49:32] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/vishalbajaj2000\n",
    "XGBOOST_PARAM = {\n",
    "    'random_state' : 1981,\n",
    "    'n_estimators' : 1000, #very slow with 2000!\n",
    "    'learning_rate': 0.15,\n",
    "    'num_leaves': 36,\n",
    "    'max_depth': 6,\n",
    "    'metric' : ['auc'],\n",
    "    'boosting_type' : 'gbdt',\n",
    "    #'drop_rate' : 0.2,    ##only for DART\n",
    "    #'max_drop' : 100,    ##only for DART\n",
    "    #'objective' : 'binary',\n",
    "    'reg_alpha' : 2.03,\n",
    "    'reg_lambda' : 4.7,\n",
    "    'feature_fraction' : 0.8, #colsample_bytree\n",
    "    'feature_fraction_seed' : 1981, \n",
    "    'max_bins' : 100,\n",
    "    'min_split_gain': 0.0148,\n",
    "    'min_child_weight' : 7.835, #min_sum_hessian_in_leaf \n",
    "    'min_data_in_leaf' : 1000, #min_child_samples\n",
    "    'random_state' : 1981, # Updated from 'seed'\n",
    "    'subsample' : .912, #also known as Bagging fraction!\n",
    "    'subsample_freq' : 200, # also known as bagging frequency!\n",
    "    'boost_from_average' : False,\n",
    "    #'verbose_eval' : 5,\n",
    "    'is_unbalance' : True,\n",
    "    #'scale_pos_weight' : 10,\n",
    "    }\n",
    "\n",
    "idx = []\n",
    "idx = [i for i in range(200_000)]\n",
    "random.shuffle(idx)\n",
    "X_train = [X_train[i] for i in idx] \n",
    "Y_train = [Y_train[i] for i in idx]\n",
    "X_train = np.array(X_train) \n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "XGBGBDT = xgb.XGBClassifier(**XGBOOST_PARAM, tree_method = 'hist', n_jobs =2, verbosity=0)\n",
    "\n",
    "XGBGBDT_FIT = XGBGBDT.fit(X_train[20_000:200_000],Y_train[20_000:200_000], \n",
    "                          eval_set=[(X_train[:20_000],Y_train[:20_000])],\n",
    "                          eval_metric='auc', early_stopping_rounds=100, verbose=False)\n",
    "Y_xgb = XGBGBDT.predict_proba(Z_train)[:,1]\n",
    "df_Xt[\"xgb\"] = Y_xgb\n",
    "\n",
    "XGBGBDT = None\n",
    "\n",
    "X_train = None\n",
    "Y_train = None\n",
    "Z_train = None\n",
    "\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_Xt.values.astype('float64')[:200_000]\n",
    "Y = df_train[\"target\"].values.astype('float64')\n",
    "Z = df_Xt.values.astype('float64')[200_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = P[\"data_BatchSize\"]\n",
    "nb_batch_valid = int((200000*0.1)//batch_size)\n",
    "\n",
    "# idx = []\n",
    "# filename = \"Boosting_idx_{:03}.pickle\".format(grid_index)\n",
    "# if P_Boosting == False:\n",
    "#     idx = [i for i in range(200_000)]\n",
    "#     random.shuffle(idx)\n",
    "#     with open(filename, 'wb') as handle: pickle.dump(idx, handle)\n",
    "# with open(filename, 'rb') as handle: idx = pickle.load(handle)\n",
    "\n",
    "idx = []\n",
    "idx = [i for i in range(200_000)]\n",
    "random.shuffle(idx)\n",
    "    \n",
    "X = [X[i] for i in idx] \n",
    "Y = [Y[i] for i in idx]\n",
    "    \n",
    "XL = []\n",
    "YL = []\n",
    "nb_fold = P[\"data_NbFolds\"]\n",
    "fold_size = int(len(X)/nb_fold)\n",
    "for i in range(nb_fold):\n",
    "    XL.append(X[int(i*fold_size):int((i+1)*fold_size)])\n",
    "    YL.append(Y[int(i*fold_size):int((i+1)*fold_size)])\n",
    "\n",
    "nb_fold_valid = P[\"data_NbValidFolds\"]\n",
    "up_sampling_coeff = P[\"data_UpSampleCoeff\"]\n",
    "total_1 = [sum(YL[i]) for i in range(nb_fold)]\n",
    "total_1.sort()\n",
    "total_1 = total_1[::-1][nb_fold_valid:]\n",
    "up_sampling_max_size = int(sum(total_1)/len(total_1))\n",
    "nb_batch_upsampling = int(up_sampling_max_size//batch_size * up_sampling_coeff*(nb_fold-nb_fold_valid))\n",
    "\n",
    "indice_to_boost = []\n",
    "for _ in range(nb_fold):indice_to_boost.append([])\n",
    "# filename = \"Boosting_boost_{:03}.pickle\".format(grid_index)\n",
    "# if P_Boosting:\n",
    "#     with open(filename, 'rb') as handle: indice_to_boost = pickle.load(handle)\n",
    "# else:\n",
    "# print(len(indice_to_boost[0]))\n",
    "# print(idx[0:10]) #[145126, 87119, 48487, 167225, 14305, 113152, 22347, 187537, 13589, 98002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Santander(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        coeff = P[\"mod_LayerCoeff\"]\n",
    "        nlayers = P[\"mod_NbLayers\"]\n",
    "        init_size = P[\"mod_HiddenSize\"]\n",
    "        self.nlayers = nlayers\n",
    "        self.datasize = P[\"mod_InputSize\"]\n",
    "\n",
    "        bias=False\n",
    "        if P[\"mod_BatchNorm\"] == \"\": \n",
    "            bias=True\n",
    "        \n",
    "        self.fc1 = nn.Linear(P[\"mod_InputSize\"], int(init_size*(coeff**1)), bias=bias)\n",
    "        self.bn1 = nn.BatchNorm1d(int(init_size*(coeff**1)))\n",
    "        out_size = int(init_size*(coeff**1))\n",
    "\n",
    "        if nlayers > 1:\n",
    "            in_size = int(init_size*(coeff**1))\n",
    "            out_size = int(init_size*(coeff**2))\n",
    "            self.fc2 = nn.Linear(in_size, out_size, bias=bias)\n",
    "            self.bn2 = nn.BatchNorm1d(out_size)\n",
    "\n",
    "        if nlayers > 2:\n",
    "            in_size = int(init_size*(coeff**2))\n",
    "            out_size = int(init_size*(coeff**3))\n",
    "            self.fc3 = nn.Linear(in_size, out_size, bias=bias)\n",
    "            self.bn3 = nn.BatchNorm1d(out_size)\n",
    "    \n",
    "        if nlayers > 3:\n",
    "            in_size = int(init_size*(coeff**3))\n",
    "            out_size = int(init_size*(coeff**4))\n",
    "            self.fc4 = nn.Linear(in_size, out_size, bias=bias)\n",
    "            self.bn4 = nn.BatchNorm1d(out_size)\n",
    "\n",
    "        if nlayers > 4:\n",
    "            in_size = int(init_size*(coeff**4))\n",
    "            out_size = int(init_size*(coeff**5))\n",
    "            self.fc5 = nn.Linear(in_size, out_size, bias=bias)\n",
    "            self.bn5 = nn.BatchNorm1d(out_size)\n",
    "\n",
    "        if nlayers > 5:\n",
    "            in_size = int(init_size*(coeff**5))\n",
    "            out_size = int(init_size*(coeff**6))\n",
    "            self.fc6 = nn.Linear(in_size, out_size, bias=bias)\n",
    "            self.bn6 = nn.BatchNorm1d(out_size)\n",
    "\n",
    "        if nlayers > 6:\n",
    "            in_size = int(init_size*(coeff**6))\n",
    "            out_size = int(init_size*(coeff**7))\n",
    "            self.fc7 = nn.Linear(in_size, out_size, bias=bias)\n",
    "            self.bn7 = nn.BatchNorm1d(out_size)\n",
    "            \n",
    "        self.fo = nn.Linear(out_size, 1)\n",
    "\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "        if P[\"mod_Act\"] == \"leaky\":\n",
    "            self.activation = nn.LeakyReLU(negative_slope=P[\"mod_ActSlope\"])\n",
    "        if P[\"mod_Act\"] == \"tanh\":\n",
    "            self.activation = nn.Tanh()\n",
    "        if P[\"mod_Act\"] == \"sig\":\n",
    "            self.activation = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(p=P[\"mod_Dropout\"])\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        nlayers = self.nlayers\n",
    "        x = x.narrow(1,0,self.datasize)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        if P[\"mod_BatchNorm\"] == \"before\":\n",
    "            x = self.bn1(x)            \n",
    "        x = self.activation(x)\n",
    "        if P[\"mod_BatchNorm\"] == \"after\":\n",
    "            x = self.bn1(x)            \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        if nlayers > 1:\n",
    "            x = self.fc2(x)\n",
    "            if P[\"mod_BatchNorm\"] == \"before\":\n",
    "                x = self.bn2(x)            \n",
    "            x = self.activation(x)\n",
    "            if P[\"mod_BatchNorm\"] == \"after\":\n",
    "                x = self.bn2(x)            \n",
    "            x = self.dropout(x)\n",
    "\n",
    "        if nlayers > 2:\n",
    "            x = self.fc3(x)\n",
    "            if P[\"mod_BatchNorm\"] == \"before\":\n",
    "                x = self.bn3(x)            \n",
    "            x = self.activation(x)\n",
    "            if P[\"mod_BatchNorm\"] == \"after\":\n",
    "                x = self.bn3(x)            \n",
    "            x = self.dropout(x)\n",
    "\n",
    "        if nlayers > 3:\n",
    "            x = self.fc4(x)\n",
    "            if P[\"mod_BatchNorm\"] == \"before\":\n",
    "                x = self.bn4(x)            \n",
    "            x = self.activation(x)\n",
    "            if P[\"mod_BatchNorm\"] == \"after\":\n",
    "                x = self.bn4(x)            \n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        if nlayers > 4:\n",
    "            x = self.fc5(x)\n",
    "            if P[\"mod_BatchNorm\"] == \"before\":\n",
    "                x = self.bn5(x)            \n",
    "            x = self.activation(x)\n",
    "            if P[\"mod_BatchNorm\"] == \"after\":\n",
    "                x = self.bn5(x)            \n",
    "            x = self.dropout(x)\n",
    "\n",
    "        if nlayers > 5:\n",
    "            x = self.fc6(x)\n",
    "            if P[\"mod_BatchNorm\"] == \"before\":\n",
    "                x = self.bn6(x)            \n",
    "            x = self.activation(x)\n",
    "            if P[\"mod_BatchNorm\"] == \"after\":\n",
    "                x = self.bn6(x)            \n",
    "            x = self.dropout(x)\n",
    "            \n",
    "        if nlayers > 6:\n",
    "            x = self.fc7(x)\n",
    "            if P[\"mod_BatchNorm\"] == \"before\":\n",
    "                x = self.bn7(x)            \n",
    "            x = self.activation(x)\n",
    "            if P[\"mod_BatchNorm\"] == \"after\":\n",
    "                x = self.bn7(x)            \n",
    "            x = self.dropout(x)\n",
    "            \n",
    "        x = self.fo(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_kept = []\n",
    "for i in range(nb_fold):\n",
    "    models_kept.append(Santander().double())\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "if P[\"train_Loss\"] == \"bce\":\n",
    "    criterion = nn.BCELoss(reduction=\"none\")\n",
    "#     criterion = nn.BCELoss()\n",
    "if P[\"train_Loss\"] == \"mse\":\n",
    "    criterion = nn.MSELoss(reduction=\"none\")\n",
    "#     criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(models, X):\n",
    "    Y_hat = None\n",
    "    for idx in range(len(X)//batch_size+1):\n",
    "        data = X[idx*batch_size:(idx+1)*batch_size]\n",
    "        if len(data) == 0: break\n",
    "        tData = torch.DoubleTensor(data).cuda()\n",
    "        Y_hat_batch = None\n",
    "        for model_idx, model in enumerate(models):\n",
    "            model = models[model_idx]\n",
    "            model.eval()\n",
    "            model.cuda()\n",
    "            with torch.no_grad():\n",
    "                predict = model(tData)\n",
    "                predict = predict.cpu()\n",
    "            if model_idx == 0 : Y_hat_batch = predict.data\n",
    "            else: Y_hat_batch += predict.data\n",
    "        Y_hat_batch = Y_hat_batch/len(models)\n",
    "        if idx == 0 : Y_hat = Y_hat_batch.numpy()\n",
    "        else: Y_hat = np.vstack((Y_hat, Y_hat_batch.numpy()))\n",
    "    return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------1\n",
      "!M:0* Epoch:  1  Tr: 0.6372  Va: 0.49235 0.49235 0.49235  auc 0.8198 0.0000  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:1* Epoch:  1  Tr: 0.6286  Va: 0.48641 0.48641 0.48641  auc 0.8246 0.0000  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:2* Epoch:  1  Tr: 0.6246  Va: 0.48947 0.48947 0.48947  auc 0.8140 0.0000  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:3* Epoch:  1  Tr: 0.6308  Va: 0.49733 0.49733 0.49733  auc 0.8177 0.0000  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:4* Epoch:  1  Tr: 0.6267  Va: 0.49248 0.49248 0.49248  auc 0.8264 0.0000  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:5* Epoch:  1  Tr: 0.6295  Va: 0.47052 0.47052 0.47052  auc 0.8158 0.0000  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:6* Epoch:  1  Tr: 0.6179  Va: 0.46408 0.46408 0.46408  auc 0.8303 0.0000  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:7* Epoch:  1  Tr: 0.6298  Va: 0.50120 0.50120 0.50120  auc 0.8235 0.0000  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:8* Epoch:  1  Tr: 0.6291  Va: 0.44711 0.44711 0.44711  auc 0.8211 0.0000  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:9* Epoch:  1  Tr: 0.6255  Va: 0.45239 0.45239 0.45239  auc 0.8206 0.0000  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:0* Epoch:  2  Tr: 0.5236  Va: 0.41968 0.41968 0.41968  auc 0.8507 0.8691  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:1* Epoch:  2  Tr: 0.5258  Va: 0.42944 0.42944 0.42944  auc 0.8532 0.8691  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:2* Epoch:  2  Tr: 0.5258  Va: 0.43427 0.43427 0.43427  auc 0.8462 0.8691  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:3* Epoch:  2  Tr: 0.5202  Va: 0.43847 0.43847 0.43847  auc 0.8470 0.8691  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:4* Epoch:  2  Tr: 0.5190  Va: 0.43102 0.43102 0.43102  auc 0.8536 0.8691  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:5* Epoch:  2  Tr: 0.5259  Va: 0.43519 0.43519 0.43519  auc 0.8501 0.8691  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:6* Epoch:  2  Tr: 0.5215  Va: 0.43199 0.43199 0.43199  auc 0.8517 0.8691  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:7* Epoch:  2  Tr: 0.5267  Va: 0.41600 0.41600 0.41600  auc 0.8543 0.8691  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:8* Epoch:  2  Tr: 0.5263  Va: 0.40891 0.40891 0.40891  auc 0.8517 0.8691  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:9* Epoch:  2  Tr: 0.5203  Va: 0.41472 0.41472 0.41472  auc 0.8475 0.8691  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:0* Epoch:  3  Tr: 0.4963  Va: 0.41086 0.41086 0.41086  auc 0.8566 0.8741  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:1* Epoch:  3  Tr: 0.4964  Va: 0.43286 0.42944 0.42944  auc 0.8606 0.8741  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:2* Epoch:  3  Tr: 0.4934  Va: 0.42101 0.42101 0.42101  auc 0.8541 0.8741  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:3* Epoch:  3  Tr: 0.4946  Va: 0.43707 0.43707 0.43707  auc 0.8541 0.8741  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:4* Epoch:  3  Tr: 0.4978  Va: 0.42295 0.42295 0.42295  auc 0.8588 0.8741  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:5* Epoch:  3  Tr: 0.4950  Va: 0.39790 0.39790 0.39790  auc 0.8572 0.8741  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:6* Epoch:  3  Tr: 0.4955  Va: 0.42615 0.42615 0.42615  auc 0.8564 0.8741  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:7* Epoch:  3  Tr: 0.4934  Va: 0.41353 0.41353 0.41353  auc 0.8605 0.8741  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:8* Epoch:  3  Tr: 0.4981  Va: 0.39490 0.39490 0.39490  auc 0.8593 0.8741  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:9* Epoch:  3  Tr: 0.4952  Va: 0.39864 0.39864 0.39864  auc 0.8545 0.8741  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:0* Epoch:  4  Tr: 0.4850  Va: 0.40543 0.40543 0.40543  auc 0.8590 0.8766  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:1* Epoch:  4  Tr: 0.4848  Va: 0.41982 0.41982 0.41982  auc 0.8627 0.8766  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:2* Epoch:  4  Tr: 0.4842  Va: 0.40696 0.40696 0.40696  auc 0.8571 0.8766  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:3* Epoch:  4  Tr: 0.4830  Va: 0.43800 0.43707 0.43707  auc 0.8560 0.8766  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:4* Epoch:  4  Tr: 0.4812  Va: 0.42533 0.42295 0.42295  auc 0.8626 0.8766  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:5* Epoch:  4  Tr: 0.4858  Va: 0.40901 0.39790 0.39790  auc 0.8598 0.8766  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:6* Epoch:  4  Tr: 0.4862  Va: 0.44295 0.42615 0.42615  auc 0.8588 0.8766  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:7* Epoch:  4  Tr: 0.4761  Va: 0.40300 0.40300 0.40300  auc 0.8630 0.8766  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:8* Epoch:  4  Tr: 0.4849  Va: 0.39525 0.39490 0.39490  auc 0.8622 0.8766  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:9* Epoch:  4  Tr: 0.4800  Va: 0.41267 0.39864 0.39864  auc 0.8570 0.8766  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:0* Epoch:  5  Tr: 0.4767  Va: 0.43056 0.40543 0.40543  auc 0.8613 0.8785  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:1* Epoch:  5  Tr: 0.4730  Va: 0.41414 0.41414 0.41414  auc 0.8646 0.8785  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:2* Epoch:  5  Tr: 0.4741  Va: 0.42031 0.40696 0.40696  auc 0.8582 0.8785  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:3* Epoch:  5  Tr: 0.4732  Va: 0.42403 0.42403 0.42403  auc 0.8583 0.8785  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:4* Epoch:  5  Tr: 0.4768  Va: 0.41800 0.41800 0.41800  auc 0.8639 0.8785  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:5* Epoch:  5  Tr: 0.4714  Va: 0.40865 0.39790 0.39790  auc 0.8619 0.8785  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:6* Epoch:  5  Tr: 0.4704  Va: 0.41135 0.41135 0.41135  auc 0.8602 0.8785  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:7* Epoch:  5  Tr: 0.4763  Va: 0.40327 0.40300 0.40300  auc 0.8646 0.8785  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:8* Epoch:  5  Tr: 0.4783  Va: 0.39418 0.39418 0.39418  auc 0.8643 0.8785  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:9* Epoch:  5  Tr: 0.4780  Va: 0.40734 0.39864 0.39864  auc 0.8588 0.8785  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:0* Epoch:  6  Tr: 0.4694  Va: 0.40906 0.40543 0.40543  auc 0.8621 0.8799  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:1* Epoch:  6  Tr: 0.4718  Va: 0.41364 0.41364 0.41364  auc 0.8656 0.8799  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:2* Epoch:  6  Tr: 0.4666  Va: 0.41408 0.40696 0.40696  auc 0.8592 0.8799  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:3* Epoch:  6  Tr: 0.4669  Va: 0.40239 0.40239 0.40239  auc 0.8597 0.8799  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:4* Epoch:  6  Tr: 0.4658  Va: 0.42495 0.41800 0.41800  auc 0.8646 0.8799  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:5* Epoch:  6  Tr: 0.4688  Va: 0.41193 0.39790 0.39790  auc 0.8629 0.8799  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:6* Epoch:  6  Tr: 0.4702  Va: 0.40643 0.40643 0.40643  auc 0.8616 0.8799  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:7* Epoch:  6  Tr: 0.4708  Va: 0.42010 0.40300 0.40300  auc 0.8657 0.8799  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:8* Epoch:  6  Tr: 0.4719  Va: 0.38941 0.38941 0.38941  auc 0.8644 0.8799  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:9* Epoch:  6  Tr: 0.4663  Va: 0.40083 0.39864 0.39864  auc 0.8597 0.8799  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:0* Epoch:  7  Tr: 0.4617  Va: 0.39221 0.39221 0.39221  auc 0.8628 0.8814  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:1* Epoch:  7  Tr: 0.4618  Va: 0.43111 0.41364 0.41364  auc 0.8665 0.8814  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:2* Epoch:  7  Tr: 0.4646  Va: 0.41156 0.40696 0.40696  auc 0.8602 0.8814  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:3* Epoch:  7  Tr: 0.4645  Va: 0.42722 0.40239 0.40239  auc 0.8605 0.8814  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:4* Epoch:  7  Tr: 0.4686  Va: 0.41343 0.41343 0.41343  auc 0.8659 0.8814  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:5* Epoch:  7  Tr: 0.4698  Va: 0.39176 0.39176 0.39176  auc 0.8638 0.8814  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:6* Epoch:  7  Tr: 0.4636  Va: 0.39925 0.39925 0.39925  auc 0.8624 0.8814  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:7* Epoch:  7  Tr: 0.4679  Va: 0.39669 0.39669 0.39669  auc 0.8666 0.8814  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:8* Epoch:  7  Tr: 0.4677  Va: 0.38808 0.38808 0.38808  auc 0.8656 0.8814  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:9* Epoch:  7  Tr: 0.4654  Va: 0.40051 0.39864 0.39864  auc 0.8607 0.8814  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:0* Epoch:  8  Tr: 0.4656  Va: 0.41821 0.39221 0.39221  auc 0.8631 0.8827  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:1* Epoch:  8  Tr: 0.4634  Va: 0.42424 0.41364 0.41364  auc 0.8670 0.8827  [5,5,5,5,5,5,5,5,5,5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!M:2* Epoch:  8  Tr: 0.4618  Va: 0.40147 0.40147 0.40147  auc 0.8607 0.8827  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:3  Epoch:  8  Tr: 0.4581  Va: 0.41134 0.40239 0.40239  auc 0.8605 0.8827  [5,5,5,4,5,5,5,5,5,5]\n",
      "!M:4* Epoch:  8  Tr: 0.4635  Va: 0.42215 0.41343 0.41343  auc 0.8661 0.8827  [5,5,5,4,5,5,5,5,5,5]\n",
      "!M:5* Epoch:  8  Tr: 0.4624  Va: 0.41298 0.39176 0.39176  auc 0.8641 0.8827  [5,5,5,4,5,5,5,5,5,5]\n",
      "!M:6* Epoch:  8  Tr: 0.4604  Va: 0.41211 0.39925 0.39925  auc 0.8626 0.8827  [5,5,5,4,5,5,5,5,5,5]\n",
      "!M:7* Epoch:  8  Tr: 0.4639  Va: 0.38784 0.38784 0.38784  auc 0.8673 0.8827  [5,5,5,4,5,5,5,5,5,5]\n",
      "!M:8* Epoch:  8  Tr: 0.4623  Va: 0.38111 0.38111 0.38111  auc 0.8665 0.8827  [5,5,5,4,5,5,5,5,5,5]\n",
      "!M:9* Epoch:  8  Tr: 0.4608  Va: 0.39777 0.39777 0.39777  auc 0.8610 0.8827  [5,5,5,4,5,5,5,5,5,5]\n",
      "!M:0* Epoch:  9  Tr: 0.4610  Va: 0.41784 0.39221 0.39221  auc 0.8637 0.8838  [5,5,5,4,5,5,5,5,5,5]\n",
      "!M:1  Epoch:  9  Tr: 0.4628  Va: 0.41228 0.41228 0.41228  auc 0.8667 0.8838  [5,4,5,4,5,5,5,5,5,5]\n",
      "!M:2* Epoch:  9  Tr: 0.4597  Va: 0.40649 0.40147 0.40147  auc 0.8618 0.8838  [5,4,5,4,5,5,5,5,5,5]\n",
      "!M:3* Epoch:  9  Tr: 0.4599  Va: 0.40063 0.40063 0.40063  auc 0.8611 0.8838  [5,4,5,5,5,5,5,5,5,5]\n",
      "!M:4* Epoch:  9  Tr: 0.4604  Va: 0.41213 0.41213 0.41213  auc 0.8666 0.8838  [5,4,5,5,5,5,5,5,5,5]\n",
      "!M:5* Epoch:  9  Tr: 0.4569  Va: 0.39308 0.39176 0.39176  auc 0.8647 0.8838  [5,4,5,5,5,5,5,5,5,5]\n",
      "!M:6* Epoch:  9  Tr: 0.4543  Va: 0.39914 0.39914 0.39914  auc 0.8633 0.8838  [5,4,5,5,5,5,5,5,5,5]\n",
      "!M:7* Epoch:  9  Tr: 0.4585  Va: 0.40523 0.38784 0.38784  auc 0.8677 0.8838  [5,4,5,5,5,5,5,5,5,5]\n",
      "!M:8  Epoch:  9  Tr: 0.4592  Va: 0.38400 0.38111 0.38111  auc 0.8664 0.8838  [5,4,5,5,5,5,5,5,4,5]\n",
      "!M:9* Epoch:  9  Tr: 0.4598  Va: 0.38829 0.38829 0.38829  auc 0.8614 0.8838  [5,4,5,5,5,5,5,5,4,5]\n",
      "!M:0* Epoch: 10  Tr: 0.4540  Va: 0.40505 0.39221 0.39221  auc 0.8640 0.8849  [5,4,5,5,5,5,5,5,4,5]\n",
      "!M:1  Epoch: 10  Tr: 0.4584  Va: 0.39348 0.39348 0.39348  auc 0.8668 0.8849  [5,3,5,5,5,5,5,5,4,5]\n",
      "!M:2* Epoch: 10  Tr: 0.4579  Va: 0.40551 0.40147 0.40147  auc 0.8620 0.8849  [5,3,5,5,5,5,5,5,4,5]\n",
      "!M:3* Epoch: 10  Tr: 0.4525  Va: 0.41110 0.40063 0.40063  auc 0.8618 0.8849  [5,3,5,5,5,5,5,5,4,5]\n",
      "!M:4* Epoch: 10  Tr: 0.4539  Va: 0.40749 0.40749 0.40749  auc 0.8670 0.8849  [5,3,5,5,5,5,5,5,4,5]\n",
      "!M:5* Epoch: 10  Tr: 0.4569  Va: 0.39879 0.39176 0.39176  auc 0.8657 0.8849  [5,3,5,5,5,5,5,5,4,5]\n",
      "!M:6* Epoch: 10  Tr: 0.4549  Va: 0.41070 0.39914 0.39914  auc 0.8637 0.8849  [5,3,5,5,5,5,5,5,4,5]\n",
      "!M:7* Epoch: 10  Tr: 0.4573  Va: 0.39978 0.38784 0.38784  auc 0.8682 0.8849  [5,3,5,5,5,5,5,5,4,5]\n",
      "!M:8* Epoch: 10  Tr: 0.4554  Va: 0.38656 0.38111 0.38111  auc 0.8670 0.8849  [5,3,5,5,5,5,5,5,5,5]\n",
      "!M:9* Epoch: 10  Tr: 0.4545  Va: 0.38418 0.38418 0.38418  auc 0.8619 0.8849  [5,3,5,5,5,5,5,5,5,5]\n",
      "!M:0* Epoch: 11  Tr: 0.4512  Va: 0.39116 0.39116 0.39116  auc 0.8644 0.8862  [5,3,5,5,5,5,5,5,5,5]\n",
      "!M:1* Epoch: 11  Tr: 0.4555  Va: 0.39216 0.39216 0.39216  auc 0.8676 0.8862  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:2* Epoch: 11  Tr: 0.4540  Va: 0.40758 0.40147 0.40147  auc 0.8623 0.8862  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:3* Epoch: 11  Tr: 0.4525  Va: 0.40270 0.40063 0.40063  auc 0.8621 0.8862  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:4* Epoch: 11  Tr: 0.4553  Va: 0.39730 0.39730 0.39730  auc 0.8675 0.8862  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:5* Epoch: 11  Tr: 0.4508  Va: 0.38267 0.38267 0.38267  auc 0.8659 0.8862  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:6* Epoch: 11  Tr: 0.4505  Va: 0.40243 0.39914 0.39914  auc 0.8641 0.8862  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:7* Epoch: 11  Tr: 0.4480  Va: 0.39320 0.38784 0.38784  auc 0.8685 0.8862  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:8* Epoch: 11  Tr: 0.4524  Va: 0.38502 0.38111 0.38111  auc 0.8679 0.8862  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:9* Epoch: 11  Tr: 0.4511  Va: 0.36345 0.36345 0.36345  auc 0.8625 0.8862  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:0* Epoch: 12  Tr: 0.4486  Va: 0.39774 0.39116 0.39116  auc 0.8645 0.8877  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:1  Epoch: 12  Tr: 0.4519  Va: 0.38983 0.38983 0.38983  auc 0.8675 0.8877  [5,4,5,5,5,5,5,5,5,5]\n",
      "!M:2* Epoch: 12  Tr: 0.4501  Va: 0.38747 0.38747 0.38747  auc 0.8627 0.8877  [5,4,5,5,5,5,5,5,5,5]\n",
      "!M:3* Epoch: 12  Tr: 0.4487  Va: 0.39135 0.39135 0.39135  auc 0.8625 0.8877  [5,4,5,5,5,5,5,5,5,5]\n",
      "!M:4  Epoch: 12  Tr: 0.4496  Va: 0.40809 0.39730 0.39730  auc 0.8671 0.8877  [5,4,5,5,4,5,5,5,5,5]\n",
      "!M:5* Epoch: 12  Tr: 0.4492  Va: 0.39826 0.38267 0.38267  auc 0.8664 0.8877  [5,4,5,5,4,5,5,5,5,5]\n",
      "!M:6  Epoch: 12  Tr: 0.4518  Va: 0.37800 0.37800 0.37800  auc 0.8640 0.8877  [5,4,5,5,4,5,4,5,5,5]\n",
      "!M:7* Epoch: 12  Tr: 0.4564  Va: 0.38835 0.38784 0.38784  auc 0.8688 0.8877  [5,4,5,5,4,5,4,5,5,5]\n",
      "!M:8* Epoch: 12  Tr: 0.4569  Va: 0.39638 0.38111 0.38111  auc 0.8681 0.8877  [5,4,5,5,4,5,4,5,5,5]\n",
      "!M:9* Epoch: 12  Tr: 0.4516  Va: 0.37981 0.36345 0.36345  auc 0.8630 0.8877  [5,4,5,5,4,5,4,5,5,5]\n",
      "!M:0* Epoch: 13  Tr: 0.4463  Va: 0.40096 0.39116 0.39116  auc 0.8649 0.8886  [5,4,5,5,4,5,4,5,5,5]\n",
      "!M:1* Epoch: 13  Tr: 0.4506  Va: 0.38715 0.38715 0.38715  auc 0.8681 0.8886  [5,5,5,5,4,5,4,5,5,5]\n",
      "!M:2* Epoch: 13  Tr: 0.4512  Va: 0.39642 0.38747 0.38747  auc 0.8631 0.8886  [5,5,5,5,4,5,4,5,5,5]\n",
      "!M:3* Epoch: 13  Tr: 0.4458  Va: 0.39598 0.39135 0.39135  auc 0.8629 0.8886  [5,5,5,5,4,5,4,5,5,5]\n",
      "!M:4* Epoch: 13  Tr: 0.4448  Va: 0.40262 0.39730 0.39730  auc 0.8676 0.8886  [5,5,5,5,5,5,4,5,5,5]\n",
      "!M:5* Epoch: 13  Tr: 0.4503  Va: 0.38625 0.38267 0.38267  auc 0.8671 0.8886  [5,5,5,5,5,5,4,5,5,5]\n",
      "!M:6* Epoch: 13  Tr: 0.4485  Va: 0.38822 0.37800 0.37800  auc 0.8648 0.8886  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:7* Epoch: 13  Tr: 0.4462  Va: 0.39038 0.38784 0.38784  auc 0.8689 0.8886  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:8* Epoch: 13  Tr: 0.4500  Va: 0.37407 0.37407 0.37407  auc 0.8682 0.8886  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:9* Epoch: 13  Tr: 0.4469  Va: 0.38009 0.36345 0.36345  auc 0.8635 0.8886  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:0* Epoch: 14  Tr: 0.4500  Va: 0.40127 0.39116 0.39116  auc 0.8650 0.8901  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:1* Epoch: 14  Tr: 0.4444  Va: 0.38096 0.38096 0.38096  auc 0.8683 0.8901  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:2* Epoch: 14  Tr: 0.4456  Va: 0.39923 0.38747 0.38747  auc 0.8632 0.8901  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:3* Epoch: 14  Tr: 0.4456  Va: 0.38948 0.38948 0.38948  auc 0.8633 0.8901  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:4* Epoch: 14  Tr: 0.4463  Va: 0.39003 0.39003 0.39003  auc 0.8680 0.8901  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:5* Epoch: 14  Tr: 0.4443  Va: 0.39983 0.38267 0.38267  auc 0.8671 0.8901  [5,5,5,5,5,5,5,5,5,5]\n",
      "!M:6  Epoch: 14  Tr: 0.4466  Va: 0.38804 0.37800 0.37800  auc 0.8647 0.8901  [5,5,5,5,5,5,4,5,5,5]\n",
      "!M:7* Epoch: 14  Tr: 0.4457  Va: 0.37636 0.37636 0.37636  auc 0.8690 0.8901  [5,5,5,5,5,5,4,5,5,5]\n",
      "!M:8* Epoch: 14  Tr: 0.4487  Va: 0.39604 0.37407 0.37407  auc 0.8690 0.8901  [5,5,5,5,5,5,4,5,5,5]\n",
      "!M:9* Epoch: 14  Tr: 0.4462  Va: 0.37522 0.36345 0.36345  auc 0.8636 0.8901  [5,5,5,5,5,5,4,5,5,5]\n",
      "!M:0* Epoch: 15  Tr: 0.4441  Va: 0.40658 0.39116 0.39116  auc 0.8655 0.8912  [5,5,5,5,5,5,4,5,5,5]\n",
      "!M:1* Epoch: 15  Tr: 0.4456  Va: 0.38091 0.38091 0.38091  auc 0.8684 0.8912  [5,5,5,5,5,5,4,5,5,5]\n",
      "!M:2* Epoch: 15  Tr: 0.4469  Va: 0.39630 0.38747 0.38747  auc 0.8638 0.8912  [5,5,5,5,5,5,4,5,5,5]\n",
      "!M:3* Epoch: 15  Tr: 0.4495  Va: 0.38391 0.38391 0.38391  auc 0.8636 0.8912  [5,5,5,5,5,5,4,5,5,5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!M:4  Epoch: 15  Tr: 0.4455  Va: 0.40124 0.39003 0.39003  auc 0.8677 0.8912  [5,5,5,5,4,5,4,5,5,5]\n",
      "!M:5* Epoch: 15  Tr: 0.4447  Va: 0.38385 0.38267 0.38267  auc 0.8672 0.8912  [5,5,5,5,4,5,4,5,5,5]\n",
      "!M:6* Epoch: 15  Tr: 0.4464  Va: 0.38717 0.37800 0.37800  auc 0.8652 0.8912  [5,5,5,5,4,5,5,5,5,5]\n",
      "!M:7* Epoch: 15  Tr: 0.4396  Va: 0.37787 0.37636 0.37636  auc 0.8695 0.8912  [5,5,5,5,4,5,5,5,5,5]\n",
      "!M:8* Epoch: 15  Tr: 0.4455  Va: 0.38335 0.37407 0.37407  auc 0.8691 0.8912  [5,5,5,5,4,5,5,5,5,5]\n",
      "!M:9  Epoch: 15  Tr: 0.4425  Va: 0.37092 0.36345 0.36345  auc 0.8634 0.8912  [5,5,5,5,4,5,5,5,5,4]\n",
      "!M:0* Epoch: 16  Tr: 0.4417  Va: 0.39315 0.39116 0.39116  auc 0.8660 0.8924  [5,5,5,5,4,5,5,5,5,4]\n",
      "!M:1* Epoch: 16  Tr: 0.4459  Va: 0.38391 0.38091 0.38091  auc 0.8689 0.8924  [5,5,5,5,4,5,5,5,5,4]\n",
      "!M:2* Epoch: 16  Tr: 0.4417  Va: 0.38824 0.38747 0.38747  auc 0.8640 0.8924  [5,5,5,5,4,5,5,5,5,4]\n",
      "!M:3* Epoch: 16  Tr: 0.4427  Va: 0.40764 0.38391 0.38391  auc 0.8639 0.8924  [5,5,5,5,4,5,5,5,5,4]\n",
      "!M:4  Epoch: 16  Tr: 0.4441  Va: 0.39555 0.39003 0.39003  auc 0.8679 0.8924  [5,5,5,5,3,5,5,5,5,4]\n",
      "!M:5* Epoch: 16  Tr: 0.4435  Va: 0.39598 0.38267 0.38267  auc 0.8677 0.8924  [5,5,5,5,3,5,5,5,5,4]\n",
      "!M:6  Epoch: 16  Tr: 0.4453  Va: 0.39094 0.37800 0.37800  auc 0.8650 0.8924  [5,5,5,5,3,5,4,5,5,4]\n",
      "!M:7* Epoch: 16  Tr: 0.4396  Va: 0.38011 0.37636 0.37636  auc 0.8696 0.8924  [5,5,5,5,3,5,4,5,5,4]\n",
      "!M:8  Epoch: 16  Tr: 0.4451  Va: 0.37668 0.37407 0.37407  auc 0.8689 0.8924  [5,5,5,5,3,5,4,5,4,4]\n",
      "!M:9* Epoch: 16  Tr: 0.4433  Va: 0.38592 0.36345 0.36345  auc 0.8643 0.8924  [5,5,5,5,3,5,4,5,4,5]\n",
      "!M:0* Epoch: 17  Tr: 0.4424  Va: 0.37955 0.37955 0.37955  auc 0.8662 0.8934  [5,5,5,5,3,5,4,5,4,5]\n",
      "!M:1* Epoch: 17  Tr: 0.4448  Va: 0.39159 0.38091 0.38091  auc 0.8689 0.8934  [5,5,5,5,3,5,4,5,4,5]\n",
      "!M:2* Epoch: 17  Tr: 0.4409  Va: 0.38186 0.38186 0.38186  auc 0.8644 0.8934  [5,5,5,5,3,5,4,5,4,5]\n",
      "!M:3* Epoch: 17  Tr: 0.4378  Va: 0.38351 0.38351 0.38351  auc 0.8643 0.8934  [5,5,5,5,3,5,4,5,4,5]\n",
      "!M:4* Epoch: 17  Tr: 0.4391  Va: 0.38164 0.38164 0.38164  auc 0.8688 0.8934  [5,5,5,5,5,5,4,5,4,5]\n",
      "!M:5  Epoch: 17  Tr: 0.4398  Va: 0.38446 0.38267 0.38267  auc 0.8676 0.8934  [5,5,5,5,5,4,4,5,4,5]\n",
      "!M:6  Epoch: 17  Tr: 0.4419  Va: 0.37858 0.37800 0.37800  auc 0.8652 0.8934  [5,5,5,5,5,4,3,5,4,5]\n",
      "!M:7* Epoch: 17  Tr: 0.4400  Va: 0.36097 0.36097 0.36097  auc 0.8696 0.8934  [5,5,5,5,5,4,3,5,4,5]\n",
      "!M:8* Epoch: 17  Tr: 0.4420  Va: 0.38710 0.37407 0.37407  auc 0.8693 0.8934  [5,5,5,5,5,4,3,5,5,5]\n",
      "!M:9  Epoch: 17  Tr: 0.4371  Va: 0.36877 0.36345 0.36345  auc 0.8642 0.8934  [5,5,5,5,5,4,3,5,5,4]\n",
      "!M:0  Epoch: 18  Tr: 0.4341  Va: 0.39305 0.37955 0.37955  auc 0.8661 0.8946  [4,5,5,5,5,4,3,5,5,4]\n",
      "!M:1  Epoch: 18  Tr: 0.4394  Va: 0.37373 0.37373 0.37373  auc 0.8688 0.8946  [4,4,5,5,5,4,3,5,5,4]\n",
      "!M:2  Epoch: 18  Tr: 0.4402  Va: 0.38960 0.38186 0.38186  auc 0.8643 0.8946  [4,4,4,5,5,4,3,5,5,4]\n",
      "!M:3  Epoch: 18  Tr: 0.4359  Va: 0.38461 0.38351 0.38351  auc 0.8643 0.8946  [4,4,4,4,5,4,3,5,5,4]\n",
      "!M:4  Epoch: 18  Tr: 0.4393  Va: 0.38446 0.38164 0.38164  auc 0.8688 0.8946  [4,4,4,4,4,4,3,5,5,4]\n",
      "!M:5* Epoch: 18  Tr: 0.4366  Va: 0.38494 0.38267 0.38267  auc 0.8678 0.8946  [4,4,4,4,4,5,3,5,5,4]\n",
      "!M:6* Epoch: 18  Tr: 0.4395  Va: 0.37495 0.37495 0.37495  auc 0.8654 0.8946  [4,4,4,4,4,5,5,5,5,4]\n",
      "!M:7* Epoch: 18  Tr: 0.4420  Va: 0.36953 0.36097 0.36097  auc 0.8698 0.8946  [4,4,4,4,4,5,5,5,5,4]\n",
      "!M:8* Epoch: 18  Tr: 0.4349  Va: 0.35979 0.35979 0.35979  auc 0.8693 0.8946  [4,4,4,4,4,5,5,5,5,4]\n",
      "!M:9* Epoch: 18  Tr: 0.4377  Va: 0.36658 0.36345 0.36345  auc 0.8644 0.8946  [4,4,4,4,4,5,5,5,5,5]\n",
      "!M:0  Epoch: 19  Tr: 0.4374  Va: 0.39251 0.37955 0.37955  auc 0.8660 0.8958  [3,4,4,4,4,5,5,5,5,5]\n",
      "!M:1* Epoch: 19  Tr: 0.4390  Va: 0.37910 0.37373 0.37373  auc 0.8691 0.8958  [3,5,4,4,4,5,5,5,5,5]\n",
      "!M:2* Epoch: 19  Tr: 0.4375  Va: 0.37715 0.37715 0.37715  auc 0.8646 0.8958  [3,5,5,4,4,5,5,5,5,5]\n",
      "!M:3  Epoch: 19  Tr: 0.4336  Va: 0.39120 0.38351 0.38351  auc 0.8641 0.8958  [3,5,5,3,4,5,5,5,5,5]\n",
      "!M:4* Epoch: 19  Tr: 0.4390  Va: 0.39066 0.38164 0.38164  auc 0.8688 0.8958  [3,5,5,3,5,5,5,5,5,5]\n",
      "!M:5  Epoch: 19  Tr: 0.4318  Va: 0.37936 0.37936 0.37936  auc 0.8677 0.8958  [3,5,5,3,5,4,5,5,5,5]\n",
      "!M:6* Epoch: 19  Tr: 0.4363  Va: 0.35834 0.35834 0.35834  auc 0.8658 0.8958  [3,5,5,3,5,4,5,5,5,5]\n",
      "!M:7* Epoch: 19  Tr: 0.4379  Va: 0.37420 0.36097 0.36097  auc 0.8701 0.8958  [3,5,5,3,5,4,5,5,5,5]\n",
      "!M:8* Epoch: 19  Tr: 0.4364  Va: 0.36879 0.35979 0.35979  auc 0.8693 0.8958  [3,5,5,3,5,4,5,5,5,5]\n",
      "!M:9* Epoch: 19  Tr: 0.4394  Va: 0.36796 0.36345 0.36345  auc 0.8645 0.8958  [3,5,5,3,5,4,5,5,5,5]\n",
      "!M:0* Epoch: 20  Tr: 0.4355  Va: 0.38440 0.37955 0.37955  auc 0.8663 0.8971  [5,5,5,3,5,4,5,5,5,5]\n",
      "!M:1  Epoch: 20  Tr: 0.4355  Va: 0.38558 0.37373 0.37373  auc 0.8690 0.8971  [5,4,5,3,5,4,5,5,5,5]\n",
      "!M:2* Epoch: 20  Tr: 0.4363  Va: 0.38973 0.37715 0.37715  auc 0.8648 0.8971  [5,4,5,3,5,4,5,5,5,5]\n",
      "!M:3* Epoch: 20  Tr: 0.4356  Va: 0.37490 0.37490 0.37490  auc 0.8646 0.8971  [5,4,5,5,5,4,5,5,5,5]\n",
      "!M:4* Epoch: 20  Tr: 0.4355  Va: 0.37320 0.37320 0.37320  auc 0.8690 0.8971  [5,4,5,5,5,4,5,5,5,5]\n",
      "!M:5* Epoch: 20  Tr: 0.4334  Va: 0.39028 0.37936 0.37936  auc 0.8683 0.8971  [5,4,5,5,5,5,5,5,5,5]\n",
      "!M:6* Epoch: 20  Tr: 0.4339  Va: 0.36607 0.35834 0.35834  auc 0.8659 0.8971  [5,4,5,5,5,5,5,5,5,5]\n",
      "!M:7  Epoch: 20  Tr: 0.4306  Va: 0.37351 0.36097 0.36097  auc 0.8699 0.8971  [5,4,5,5,5,5,5,4,5,5]\n",
      "!M:8  Epoch: 20  Tr: 0.4361  Va: 0.35966 0.35966 0.35966  auc 0.8691 0.8971  [5,4,5,5,5,5,5,4,4,5]\n",
      "!M:9* Epoch: 20  Tr: 0.4364  Va: 0.35588 0.35588 0.35588  auc 0.8648 0.8971  [5,4,5,5,5,5,5,4,4,5]\n",
      "!M:0  Epoch: 21  Tr: 0.4284  Va: 0.36960 0.36960 0.36960  auc 0.8662 0.8988  [4,4,5,5,5,5,5,4,4,5]\n",
      "!M:1* Epoch: 21  Tr: 0.4304  Va: 0.39060 0.37373 0.37373  auc 0.8693 0.8988  [4,5,5,5,5,5,5,4,4,5]\n",
      "!M:2* Epoch: 21  Tr: 0.4330  Va: 0.38176 0.37715 0.37715  auc 0.8651 0.8988  [4,5,5,5,5,5,5,4,4,5]\n",
      "!M:3* Epoch: 21  Tr: 0.4303  Va: 0.37026 0.37026 0.37026  auc 0.8646 0.8988  [4,5,5,5,5,5,5,4,4,5]\n",
      "!M:4  Epoch: 21  Tr: 0.4371  Va: 0.38234 0.37320 0.37320  auc 0.8690 0.8988  [4,5,5,5,4,5,5,4,4,5]\n",
      "!M:5* Epoch: 21  Tr: 0.4307  Va: 0.36033 0.36033 0.36033  auc 0.8685 0.8988  [4,5,5,5,4,5,5,4,4,5]\n",
      "!M:6* Epoch: 21  Tr: 0.4296  Va: 0.37048 0.35834 0.35834  auc 0.8660 0.8988  [4,5,5,5,4,5,5,4,4,5]\n",
      "!M:7* Epoch: 21  Tr: 0.4333  Va: 0.35725 0.35725 0.35725  auc 0.8702 0.8988  [4,5,5,5,4,5,5,5,4,5]\n",
      "!M:8  Epoch: 21  Tr: 0.4337  Va: 0.36656 0.35966 0.35966  auc 0.8690 0.8988  [4,5,5,5,4,5,5,5,3,5]\n",
      "!M:9* Epoch: 21  Tr: 0.4295  Va: 0.36430 0.35588 0.35588  auc 0.8648 0.8988  [4,5,5,5,4,5,5,5,3,5]\n",
      "!M:0* Epoch: 22  Tr: 0.4279  Va: 0.36381 0.36381 0.36381  auc 0.8664 0.9001  [5,5,5,5,4,5,5,5,3,5]\n",
      "!M:1  Epoch: 22  Tr: 0.4351  Va: 0.38337 0.37373 0.37373  auc 0.8692 0.9001  [5,4,5,5,4,5,5,5,3,5]\n",
      "!M:2* Epoch: 22  Tr: 0.4268  Va: 0.38023 0.37715 0.37715  auc 0.8652 0.9001  [5,4,5,5,4,5,5,5,3,5]\n",
      "!M:3* Epoch: 22  Tr: 0.4285  Va: 0.35522 0.35522 0.35522  auc 0.8653 0.9001  [5,4,5,5,4,5,5,5,3,5]\n",
      "!M:4  Epoch: 22  Tr: 0.4272  Va: 0.38388 0.37320 0.37320  auc 0.8686 0.9001  [5,4,5,5,3,5,5,5,3,5]\n",
      "!M:5* Epoch: 22  Tr: 0.4253  Va: 0.37248 0.36033 0.36033  auc 0.8685 0.9001  [5,4,5,5,3,5,5,5,3,5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!M:6  Epoch: 22  Tr: 0.4327  Va: 0.35448 0.35448 0.35448  auc 0.8657 0.9001  [5,4,5,5,3,5,4,5,3,5]\n",
      "!M:7* Epoch: 22  Tr: 0.4296  Va: 0.37282 0.35725 0.35725  auc 0.8703 0.9001  [5,4,5,5,3,5,4,5,3,5]\n",
      "!M:8* Epoch: 22  Tr: 0.4299  Va: 0.35545 0.35545 0.35545  auc 0.8694 0.9001  [5,4,5,5,3,5,4,5,5,5]\n",
      "!M:9  Epoch: 22  Tr: 0.4284  Va: 0.35908 0.35588 0.35588  auc 0.8644 0.9001  [5,4,5,5,3,5,4,5,5,4]\n",
      "!M:0  Epoch: 23  Tr: 0.4287  Va: 0.37919 0.36381 0.36381  auc 0.8662 0.9013  [4,4,5,5,3,5,4,5,5,4]\n",
      "!M:1* Epoch: 23  Tr: 0.4308  Va: 0.38027 0.37373 0.37373  auc 0.8696 0.9013  [4,5,5,5,3,5,4,5,5,4]\n",
      "!M:2  Epoch: 23  Tr: 0.4285  Va: 0.36496 0.36496 0.36496  auc 0.8650 0.9013  [4,5,4,5,3,5,4,5,5,4]\n",
      "!M:3  Epoch: 23  Tr: 0.4282  Va: 0.37353 0.35522 0.35522  auc 0.8651 0.9013  [4,5,4,4,3,5,4,5,5,4]\n",
      "!M:4  Epoch: 23  Tr: 0.4310  Va: 0.37605 0.37320 0.37320  auc 0.8689 0.9013  [4,5,4,4,2,5,4,5,5,4]\n",
      "!M:5* Epoch: 23  Tr: 0.4281  Va: 0.37798 0.36033 0.36033  auc 0.8689 0.9013  [4,5,4,4,2,5,4,5,5,4]\n",
      "!M:6  Epoch: 23  Tr: 0.4274  Va: 0.35354 0.35354 0.35354  auc 0.8660 0.9013  [4,5,4,4,2,5,3,5,5,4]\n",
      "!M:7  Epoch: 23  Tr: 0.4275  Va: 0.36559 0.35725 0.35725  auc 0.8700 0.9013  [4,5,4,4,2,5,3,4,5,4]\n",
      "!M:8* Epoch: 23  Tr: 0.4325  Va: 0.36325 0.35545 0.35545  auc 0.8695 0.9013  [4,5,4,4,2,5,3,4,5,4]\n",
      "!M:9* Epoch: 23  Tr: 0.4254  Va: 0.34114 0.34114 0.34114  auc 0.8651 0.9013  [4,5,4,4,2,5,3,4,5,5]\n",
      "!M:0  Epoch: 24  Tr: 0.4256  Va: 0.37375 0.36381 0.36381  auc 0.8661 0.9021  [3,5,4,4,2,5,3,4,5,5]\n",
      "!M:1  Epoch: 24  Tr: 0.4285  Va: 0.35786 0.35786 0.35786  auc 0.8693 0.9021  [3,4,4,4,2,5,3,4,5,5]\n",
      "!M:2  Epoch: 24  Tr: 0.4281  Va: 0.36747 0.36496 0.36496  auc 0.8649 0.9021  [3,4,3,4,2,5,3,4,5,5]\n",
      "!M:3* Epoch: 24  Tr: 0.4258  Va: 0.36490 0.35522 0.35522  auc 0.8654 0.9021  [3,4,3,5,2,5,3,4,5,5]\n",
      "!M:4* Epoch: 24  Tr: 0.4267  Va: 0.36677 0.36677 0.36677  auc 0.8691 0.9021  [3,4,3,5,5,5,3,4,5,5]\n",
      "!M:5  Epoch: 24  Tr: 0.4231  Va: 0.36324 0.36033 0.36033  auc 0.8685 0.9021  [3,4,3,5,5,4,3,4,5,5]\n",
      "!M:6* Epoch: 24  Tr: 0.4264  Va: 0.35836 0.35354 0.35354  auc 0.8663 0.9021  [3,4,3,5,5,4,5,4,5,5]\n",
      "!M:7* Epoch: 24  Tr: 0.4241  Va: 0.35111 0.35111 0.35111  auc 0.8706 0.9021  [3,4,3,5,5,4,5,5,5,5]\n",
      "!M:8* Epoch: 24  Tr: 0.4268  Va: 0.34863 0.34863 0.34863  auc 0.8697 0.9021  [3,4,3,5,5,4,5,5,5,5]\n",
      "!M:9* Epoch: 24  Tr: 0.4268  Va: 0.35556 0.34114 0.34114  auc 0.8652 0.9021  [3,4,3,5,5,4,5,5,5,5]\n",
      "!M:0  Epoch: 25  Tr: 0.4292  Va: 0.37605 0.36381 0.36381  auc 0.8662 0.9037  [2,4,3,5,5,4,5,5,5,5]\n",
      "!M:1  Epoch: 25  Tr: 0.4275  Va: 0.36048 0.35786 0.35786  auc 0.8694 0.9037  [2,3,3,5,5,4,5,5,5,5]\n",
      "!M:2* Epoch: 25  Tr: 0.4284  Va: 0.35439 0.35439 0.35439  auc 0.8653 0.9037  [2,3,5,5,5,4,5,5,5,5]\n",
      "!M:3* Epoch: 25  Tr: 0.4260  Va: 0.36071 0.35522 0.35522  auc 0.8655 0.9037  [2,3,5,5,5,4,5,5,5,5]\n",
      "!M:4  Epoch: 25  Tr: 0.4271  Va: 0.38463 0.36677 0.36677  auc 0.8690 0.9037  [2,3,5,5,4,4,5,5,5,5]\n",
      "!M:5* Epoch: 25  Tr: 0.4209  Va: 0.38405 0.36033 0.36033  auc 0.8689 0.9037  [2,3,5,5,4,5,5,5,5,5]\n",
      "!M:6* Epoch: 25  Tr: 0.4262  Va: 0.36219 0.35354 0.35354  auc 0.8664 0.9037  [2,3,5,5,4,5,5,5,5,5]\n",
      "!M:7  Epoch: 25  Tr: 0.4266  Va: 0.35737 0.35111 0.35111  auc 0.8705 0.9037  [2,3,5,5,4,5,5,4,5,5]\n",
      "!M:8* Epoch: 25  Tr: 0.4282  Va: 0.35519 0.34863 0.34863  auc 0.8698 0.9037  [2,3,5,5,4,5,5,4,5,5]\n",
      "!M:9  Epoch: 25  Tr: 0.4219  Va: 0.35867 0.34114 0.34114  auc 0.8651 0.9037  [2,3,5,5,4,5,5,4,5,4]\n",
      "!M:0  Epoch: 26  Tr: 0.4204  Va: 0.36585 0.36381 0.36381  auc 0.8662 0.9048  [1,3,5,5,4,5,5,4,5,4]\n",
      "!M:1  Epoch: 26  Tr: 0.4204  Va: 0.35806 0.35786 0.35786  auc 0.8692 0.9048  [1,2,5,5,4,5,5,4,5,4]\n",
      "!M:2* Epoch: 26  Tr: 0.4246  Va: 0.35304 0.35304 0.35304  auc 0.8654 0.9048  [1,2,5,5,4,5,5,4,5,4]\n",
      "!M:3  Epoch: 26  Tr: 0.4244  Va: 0.35704 0.35522 0.35522  auc 0.8654 0.9048  [1,2,5,4,4,5,5,4,5,4]\n",
      "!M:4* Epoch: 26  Tr: 0.4220  Va: 0.36238 0.36238 0.36238  auc 0.8692 0.9048  [1,2,5,4,5,5,5,4,5,4]\n",
      "!M:5* Epoch: 26  Tr: 0.4245  Va: 0.37074 0.36033 0.36033  auc 0.8689 0.9048  [1,2,5,4,5,5,5,4,5,4]\n",
      "!M:6  Epoch: 26  Tr: 0.4292  Va: 0.35394 0.35354 0.35354  auc 0.8662 0.9048  [1,2,5,4,5,5,4,4,5,4]\n",
      "!M:7  Epoch: 26  Tr: 0.4260  Va: 0.35947 0.35111 0.35111  auc 0.8701 0.9048  [1,2,5,4,5,5,4,3,5,4]\n",
      "!M:8* Epoch: 26  Tr: 0.4266  Va: 0.34257 0.34257 0.34257  auc 0.8699 0.9048  [1,2,5,4,5,5,4,3,5,4]\n",
      "!M:9  Epoch: 26  Tr: 0.4218  Va: 0.35201 0.34114 0.34114  auc 0.8651 0.9048  [1,2,5,4,5,5,4,3,5,3]\n",
      "!M:0  Epoch: 27  Tr: 0.4192  Va: 0.35754 0.35754 0.35754  auc 0.8662 0.9055  [0,2,5,4,5,5,4,3,5,3]\n",
      "!M:1* Epoch: 27  Tr: 0.4231  Va: 0.35982 0.35786 0.35786  auc 0.8698 0.9055  [0,5,5,4,5,5,4,3,5,3]\n",
      "!M:2* Epoch: 27  Tr: 0.4257  Va: 0.37566 0.35304 0.35304  auc 0.8658 0.9055  [0,5,5,4,5,5,4,3,5,3]\n",
      "!M:3* Epoch: 27  Tr: 0.4235  Va: 0.36215 0.35522 0.35522  auc 0.8656 0.9055  [0,5,5,5,5,5,4,3,5,3]\n",
      "!M:4  Epoch: 27  Tr: 0.4165  Va: 0.36352 0.36238 0.36238  auc 0.8688 0.9055  [0,5,5,5,4,5,4,3,5,3]\n",
      "!M:5  Epoch: 27  Tr: 0.4187  Va: 0.34885 0.34885 0.34885  auc 0.8685 0.9055  [0,5,5,5,4,4,4,3,5,3]\n",
      "!M:6  Epoch: 27  Tr: 0.4205  Va: 0.34761 0.34761 0.34761  auc 0.8660 0.9055  [0,5,5,5,4,4,3,3,5,3]\n",
      "!M:7  Epoch: 27  Tr: 0.4237  Va: 0.35850 0.35111 0.35111  auc 0.8702 0.9055  [0,5,5,5,4,4,3,2,5,3]\n",
      "!M:8  Epoch: 27  Tr: 0.4187  Va: 0.34366 0.34257 0.34257  auc 0.8697 0.9055  [0,5,5,5,4,4,3,2,4,3]\n",
      "!M:9* Epoch: 27  Tr: 0.4182  Va: 0.34690 0.34114 0.34114  auc 0.8658 0.9055  [0,5,5,5,4,4,3,2,4,5]\n",
      "!M:1* Epoch: 28  Tr: 0.4226  Va: 0.36374 0.35786 0.35786  auc 0.8698 0.9068  [0,5,5,5,4,4,3,2,4,5]\n",
      "!M:2* Epoch: 28  Tr: 0.4197  Va: 0.33898 0.33898 0.33898  auc 0.8659 0.9068  [0,5,5,5,4,4,3,2,4,5]\n",
      "!M:3  Epoch: 28  Tr: 0.4159  Va: 0.37141 0.35522 0.35522  auc 0.8654 0.9068  [0,5,5,4,4,4,3,2,4,5]\n",
      "!M:4  Epoch: 28  Tr: 0.4215  Va: 0.37612 0.36238 0.36238  auc 0.8689 0.9068  [0,5,5,4,3,4,3,2,4,5]\n",
      "!M:5  Epoch: 28  Tr: 0.4187  Va: 0.34375 0.34375 0.34375  auc 0.8685 0.9068  [0,5,5,4,3,3,3,2,4,5]\n",
      "!M:6  Epoch: 28  Tr: 0.4247  Va: 0.34899 0.34761 0.34761  auc 0.8662 0.9068  [0,5,5,4,3,3,2,2,4,5]\n",
      "!M:7  Epoch: 28  Tr: 0.4206  Va: 0.33550 0.33550 0.33550  auc 0.8706 0.9068  [0,5,5,4,3,3,2,1,4,5]\n",
      "!M:8  Epoch: 28  Tr: 0.4182  Va: 0.33718 0.33718 0.33718  auc 0.8692 0.9068  [0,5,5,4,3,3,2,1,3,5]\n",
      "!M:9  Epoch: 28  Tr: 0.4164  Va: 0.34405 0.34114 0.34114  auc 0.8654 0.9068  [0,5,5,4,3,3,2,1,3,4]\n",
      "!M:1  Epoch: 29  Tr: 0.4162  Va: 0.35593 0.35593 0.35593  auc 0.8697 0.9071  [0,4,5,4,3,3,2,1,3,4]\n",
      "!M:2  Epoch: 29  Tr: 0.4231  Va: 0.34525 0.33898 0.33898  auc 0.8654 0.9071  [0,4,4,4,3,3,2,1,3,4]\n",
      "!M:3* Epoch: 29  Tr: 0.4156  Va: 0.35435 0.35435 0.35435  auc 0.8657 0.9071  [0,4,4,5,3,3,2,1,3,4]\n",
      "!M:4  Epoch: 29  Tr: 0.4160  Va: 0.37068 0.36238 0.36238  auc 0.8690 0.9071  [0,4,4,5,2,3,2,1,3,4]\n",
      "!M:5  Epoch: 29  Tr: 0.4157  Va: 0.34767 0.34375 0.34375  auc 0.8686 0.9071  [0,4,4,5,2,2,2,1,3,4]\n",
      "!M:6* Epoch: 29  Tr: 0.4223  Va: 0.33684 0.33684 0.33684  auc 0.8667 0.9071  [0,4,4,5,2,2,5,1,3,4]\n",
      "!M:7  Epoch: 29  Tr: 0.4139  Va: 0.34710 0.33550 0.33550  auc 0.8702 0.9071  [0,4,4,5,2,2,5,0,3,4]\n",
      "!M:8* Epoch: 29  Tr: 0.4187  Va: 0.33613 0.33613 0.33613  auc 0.8699 0.9071  [0,4,4,5,2,2,5,0,5,4]\n",
      "!M:9  Epoch: 29  Tr: 0.4161  Va: 0.34259 0.34114 0.34114  auc 0.8656 0.9071  [0,4,4,5,2,2,5,0,5,3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!M:1  Epoch: 30  Tr: 0.4172  Va: 0.34206 0.34206 0.34206  auc 0.8695 0.9082  [0,3,4,5,2,2,5,0,5,3]\n",
      "!M:2  Epoch: 30  Tr: 0.4192  Va: 0.35288 0.33898 0.33898  auc 0.8648 0.9082  [0,3,3,5,2,2,5,0,5,3]\n",
      "!M:3  Epoch: 30  Tr: 0.4172  Va: 0.34430 0.34430 0.34430  auc 0.8648 0.9082  [0,3,3,4,2,2,5,0,5,3]\n",
      "!M:4  Epoch: 30  Tr: 0.4165  Va: 0.34910 0.34910 0.34910  auc 0.8691 0.9082  [0,3,3,4,1,2,5,0,5,3]\n",
      "!M:5  Epoch: 30  Tr: 0.4146  Va: 0.34570 0.34375 0.34375  auc 0.8687 0.9082  [0,3,3,4,1,1,5,0,5,3]\n",
      "!M:6  Epoch: 30  Tr: 0.4194  Va: 0.35165 0.33684 0.33684  auc 0.8663 0.9082  [0,3,3,4,1,1,4,0,5,3]\n",
      "!M:8  Epoch: 30  Tr: 0.4147  Va: 0.33757 0.33613 0.33613  auc 0.8697 0.9082  [0,3,3,4,1,1,4,0,4,3]\n",
      "!M:9  Epoch: 30  Tr: 0.4158  Va: 0.34864 0.34114 0.34114  auc 0.8655 0.9082  [0,3,3,4,1,1,4,0,4,2]\n",
      "!M:1  Epoch: 31  Tr: 0.4179  Va: 0.33400 0.33400 0.33400  auc 0.8693 0.9082  [0,2,3,4,1,1,4,0,4,2]\n",
      "!M:2  Epoch: 31  Tr: 0.4152  Va: 0.34489 0.33898 0.33898  auc 0.8658 0.9082  [0,2,2,4,1,1,4,0,4,2]\n",
      "!M:3  Epoch: 31  Tr: 0.4059  Va: 0.35532 0.34430 0.34430  auc 0.8648 0.9082  [0,2,2,3,1,1,4,0,4,2]\n",
      "!M:4  Epoch: 31  Tr: 0.4190  Va: 0.35302 0.34910 0.34910  auc 0.8690 0.9082  [0,2,2,3,0,1,4,0,4,2]\n",
      "!M:5  Epoch: 31  Tr: 0.4138  Va: 0.33800 0.33800 0.33800  auc 0.8688 0.9082  [0,2,2,3,0,0,4,0,4,2]\n",
      "!M:6  Epoch: 31  Tr: 0.4183  Va: 0.35542 0.33684 0.33684  auc 0.8662 0.9082  [0,2,2,3,0,0,3,0,4,2]\n",
      "!M:8  Epoch: 31  Tr: 0.4145  Va: 0.33243 0.33243 0.33243  auc 0.8693 0.9082  [0,2,2,3,0,0,3,0,3,2]\n",
      "!M:9  Epoch: 31  Tr: 0.4112  Va: 0.33256 0.33256 0.33256  auc 0.8652 0.9082  [0,2,2,3,0,0,3,0,3,1]\n",
      "!M:1  Epoch: 32  Tr: 0.4128  Va: 0.34383 0.33400 0.33400  auc 0.8695 0.9082  [0,1,2,3,0,0,3,0,3,1]\n",
      "!M:2* Epoch: 32  Tr: 0.4155  Va: 0.36165 0.33898 0.33898  auc 0.8659 0.9082  [0,1,5,3,0,0,3,0,3,1]\n",
      "!M:3  Epoch: 32  Tr: 0.4081  Va: 0.34133 0.34133 0.34133  auc 0.8652 0.9082  [0,1,5,2,0,0,3,0,3,1]\n",
      "!M:6  Epoch: 32  Tr: 0.4105  Va: 0.34508 0.33684 0.33684  auc 0.8666 0.9082  [0,1,5,2,0,0,2,0,3,1]\n",
      "!M:8  Epoch: 32  Tr: 0.4162  Va: 0.32783 0.32783 0.32783  auc 0.8693 0.9082  [0,1,5,2,0,0,2,0,2,1]\n",
      "!M:9  Epoch: 32  Tr: 0.4096  Va: 0.34247 0.33256 0.33256  auc 0.8651 0.9082  [0,1,5,2,0,0,2,0,2,0]\n",
      "!M:1  Epoch: 33  Tr: 0.4123  Va: 0.34346 0.33400 0.33400  auc 0.8696 0.9087  [0,0,5,2,0,0,2,0,2,0]\n",
      "!M:2* Epoch: 33  Tr: 0.4136  Va: 0.34334 0.33898 0.33898  auc 0.8660 0.9087  [0,0,5,2,0,0,2,0,2,0]\n",
      "!M:6  Epoch: 33  Tr: 0.4100  Va: 0.34499 0.33684 0.33684  auc 0.8663 0.9087  [0,0,5,1,0,0,1,0,2,0]\n",
      "!M:3  Epoch: 33  Tr: 0.4056  Va: 0.34261 0.34133 0.34133  auc 0.8652 0.9087  [0,0,5,1,0,0,2,0,2,0]\n",
      "!M:8  Epoch: 33  Tr: 0.4113  Va: 0.33072 0.32783 0.32783  auc 0.8694 0.9087  [0,0,5,1,0,0,1,0,1,0]\n",
      "!M:2  Epoch: 34  Tr: 0.4106  Va: 0.35972 0.33898 0.33898  auc 0.8660 0.9088  [0,0,4,1,0,0,1,0,1,0]\n",
      "!M:3  Epoch: 34  Tr: 0.4072  Va: 0.35201 0.34133 0.34133  auc 0.8650 0.9088  [0,0,4,0,0,0,1,0,1,0]\n",
      "!M:6  Epoch: 34  Tr: 0.4097  Va: 0.35447 0.33684 0.33684  auc 0.8658 0.9088  [0,0,4,0,0,0,0,0,1,0]\n",
      "!M:8  Epoch: 34  Tr: 0.4109  Va: 0.33093 0.32783 0.32783  auc 0.8697 0.9088  [0,0,4,0,0,0,0,0,0,0]\n",
      "!M:2* Epoch: 35  Tr: 0.4054  Va: 0.35037 0.33898 0.33898  auc 0.8661 0.9088  [0,0,5,0,0,0,0,0,0,0]\n",
      "!M:2  Epoch: 36  Tr: 0.4070  Va: 0.35089 0.33898 0.33898  auc 0.8659 0.9091  [0,0,4,0,0,0,0,0,0,0]\n",
      "!M:2  Epoch: 37  Tr: 0.4013  Va: 0.34744 0.33898 0.33898  auc 0.8657 0.9091  [0,0,3,0,0,0,0,0,0,0]\n",
      "!M:2  Epoch: 38  Tr: 0.4012  Va: 0.34206 0.33898 0.33898  auc 0.8654 0.9091  [0,0,2,0,0,0,0,0,0,0]\n",
      "!M:2  Epoch: 39  Tr: 0.4002  Va: 0.32940 0.32940 0.32940  auc 0.8656 0.9091  [0,0,1,0,0,0,0,0,0,0]\n",
      "!M:2  Epoch: 40  Tr: 0.3992  Va: 0.33517 0.32940 0.32940  auc 0.8652 0.9091  [0,0,0,0,0,0,0,0,0,0]\n",
      "------------2\n",
      " M:0  Epoch:  1  Tr: 0.6273  Va: 0.47798 0.47798 0.35754  auc 0.8251 0.9091  [5,5,5,5,5,5,5,5,5,5]\n",
      " M:1  Epoch:  1  Tr: 0.6256  Va: 0.46005 0.46005 0.33400  auc 0.8318 0.9091  [5,5,5,5,5,5,5,5,5,5]\n",
      " M:2  Epoch:  1  Tr: 0.6249  Va: 0.45004 0.45004 0.32940  auc 0.8184 0.9091  [5,5,5,5,5,5,5,5,5,5]\n",
      " M:3  Epoch:  1  Tr: 0.6312  Va: 0.44607 0.44607 0.34133  auc 0.8177 0.9091  [5,5,5,5,5,5,5,5,5,5]\n",
      " M:4  Epoch:  1  Tr: 0.6339  Va: 0.49007 0.49007 0.34910  auc 0.8164 0.9091  [5,5,5,5,5,5,5,5,5,5]\n",
      " M:5  Epoch:  1  Tr: 0.6241  Va: 0.47978 0.47978 0.33800  auc 0.8273 0.9091  [5,5,5,5,5,5,5,5,5,5]\n",
      " M:6  Epoch:  1  Tr: 0.6356  Va: 0.47631 0.47631 0.33684  auc 0.8143 0.9091  [5,5,5,5,5,5,5,5,5,5]\n",
      " M:7  Epoch:  1  Tr: 0.6418  Va: 0.48260 0.48260 0.33550  auc 0.8153 0.9091  [5,5,5,5,5,5,5,5,5,5]\n",
      " M:8  Epoch:  1  Tr: 0.6140  Va: 0.45207 0.45207 0.32783  auc 0.8375 0.9091  [5,5,5,5,5,5,5,5,5,5]\n",
      " M:9  Epoch:  1  Tr: 0.6177  Va: 0.45600 0.45600 0.33256  auc 0.8317 0.9091  [5,5,5,5,5,5,5,5,5,5]\n",
      " M:0  Epoch:  2  Tr: 0.5222  Va: 0.42599 0.42599 0.35754  auc 0.8531 0.9091  [5,5,5,5,5,5,5,5,5,5]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_epochs = P[\"train_MaxEpoch\"]\n",
    "countdown_init = P[\"train_CountDown\"]\n",
    "valid_loss_global_value = [1_000_000.0 for _ in range(nb_fold)]\n",
    "valid_metric_global_value = [1_000_000.0 for _ in range(nb_fold)]\n",
    "Lambda_L1 = P[\"train_LbdL1\"]\n",
    "Lambda_L2 = P[\"train_LbdL2\"]\n",
    "Lambda_AUC = P[\"train_LbdAUC\"]\n",
    "recaculate_roc = True\n",
    "roc = 0.0\n",
    "for model_loop in range(P[\"train_MaxLoop\"]):\n",
    "    print(\"-\"*12+str(model_loop+1))\n",
    "    found_a_better_model = False\n",
    "    roc_value = 0.0\n",
    "    countdown = []\n",
    "    valid_loss_global = 10000.0\n",
    "    roc_valid_value = 0.0\n",
    "    valid_loss_local_value = [1_000_000.0 for _ in range(nb_fold)]\n",
    "    valid_metric_local_value = [1_000_000.0 for _ in range(nb_fold)]\n",
    "    \n",
    "    models = []\n",
    "    for i in range(nb_fold):\n",
    "        models.append(Santander().double())\n",
    "    optimizers = []\n",
    "    for model in models : \n",
    "        if P[\"train_Opt\"] == \"SGD\":\n",
    "            optimizers.append(torch.optim.SGD(model.parameters(), lr=P[\"train_LearnRate\"], \n",
    "                                              momentum=P[\"train_SGDmomentum\"], nesterov=True, \n",
    "                                              weight_decay=Lambda_L2))\n",
    "        if P[\"train_Opt\"] == \"Adam\":\n",
    "            optimizers.append(torch.optim.Adam(model.parameters(), lr=P[\"train_LearnRate\"], \n",
    "                                              weight_decay=Lambda_L2))\n",
    "    \n",
    "    for model in models :\n",
    "        countdown.append(countdown_init)\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "\n",
    "        for model_idx, model in enumerate(models):\n",
    "            if countdown[model_idx] > 0:\n",
    "\n",
    "                X_train, Y_train = [], []\n",
    "                X_valid, Y_valid = [], []\n",
    "                valid_selected = []\n",
    "                for j in range(P[\"data_NbValidFolds\"]):\n",
    "                    valid_selected.append((model_idx+j)%len(models))\n",
    "                    X_valid += XL[(model_idx+j)%len(models)]\n",
    "                    Y_valid += YL[(model_idx+j)%len(models)]\n",
    "                train_selected = [j for j in range(len(models)) if j not in(valid_selected)]\n",
    "                for j in train_selected:\n",
    "                    X_train += XL[(j)%len(models)]\n",
    "                    Y_train += YL[(j)%len(models)]\n",
    "                    if P_Boosting == True:\n",
    "                        X_train += [XL[(j)%len(models)][i] for i in indice_to_boost[(j)%len(models)]]\n",
    "                        Y_train += [YL[(j)%len(models)][i] for i in indice_to_boost[(j)%len(models)]]\n",
    "\n",
    "                idx_0 = [i for i,y in enumerate(Y_train) if y == 0]\n",
    "                idx_1 = [i for i,y in enumerate(Y_train) if y == 1]\n",
    "\n",
    "                #downsampling\n",
    "                random.shuffle(idx_0)\n",
    "                idx_0 = idx_0[:int(len(idx_0)*P[\"data_DownSampleCoeff\"])]\n",
    "                #idx_0 = idx_0[:(len(idx_0)//batch_size)*batch_size]\n",
    "                X_train = [X_train[i] for i in idx_0] + [X_train[i] for i in idx_1]\n",
    "                Y_train = [Y_train[i] for i in idx_0] + [Y_train[i] for i in idx_1]\n",
    "\n",
    "                #upsampling\n",
    "                idx_1 = [i for i,y in enumerate(Y_train) if y == 1]\n",
    "                random.shuffle(idx_1)\n",
    "                idx_temp = []\n",
    "                for i in range(int(up_sampling_coeff+.999)):\n",
    "                    idx_temp += idx_1\n",
    "                idx_1 = idx_temp\n",
    "                idx_temp = None\n",
    "                X_up = [X_train[i] for i in idx_1[:nb_batch_upsampling*batch_size]]\n",
    "                Y_up = [Y_train[i] for i in idx_1[:nb_batch_upsampling*batch_size]]\n",
    "                X_train += X_up\n",
    "                Y_train += Y_up\n",
    "\n",
    "                #shuffling\n",
    "                idx_loc = [i for i in range(len(X_train))]\n",
    "                random.shuffle(idx_loc)\n",
    "                X_train = [X_train[i] for i in idx_loc] \n",
    "                Y_train = [Y_train[i] for i in idx_loc]            \n",
    "\n",
    "                optimizer = optimizers[model_idx]\n",
    "                train_loss = 0.0\n",
    "                valid_loss = 0.0\n",
    "                nb_batch_train = (200_000-fold_size*P[\"data_NbValidFolds\"])//batch_size+nb_batch_upsampling \n",
    "                #-nb_batch_valid #+nb_batch_1*1\n",
    "                nb_batch_train = len(X_train)//batch_size\n",
    "                model.cuda()\n",
    "\n",
    "                for idx in range(nb_batch_train):\n",
    "                    model.train()\n",
    "                    data = X_train[idx*batch_size:(idx+1)*batch_size]\n",
    "                    target = Y_train[idx*batch_size:(idx+1)*batch_size]\n",
    "                    tData = torch.DoubleTensor(data)\n",
    "                    tTarget = torch.DoubleTensor(target)\n",
    "                    tData, tTarget = tData.cuda(), tTarget.cuda() \n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(tData)\n",
    "                    loss = criterion(outputs, tTarget.reshape(-1,1))\n",
    "                    loss = loss + loss * tTarget.reshape(-1,1) * P[\"train_AugLossAlpha\"]\n",
    "#                     loss = loss.sum() / (1+P[\"train_AugLossAlpha\"])\n",
    "                    loss = loss*(1+P[\"train_AugLossAlpha\"])\n",
    "\n",
    "                    if Lambda_AUC > 0 :\n",
    "                        roc_train = roc_auc_score(tTarget.reshape(-1,1).clone().cpu().data, outputs.clone().cpu().data)\n",
    "                        AUC_loss = Lambda_AUC*((1-roc_train)/roc_train)\n",
    "                        loss += loss.clone()*AUC_loss\n",
    "                    L1_loss = 0\n",
    "                    if Lambda_L1 > 0 :\n",
    "                        for param in model.parameters():\n",
    "                            L1_loss += torch.sum(torch.abs(param))\n",
    "                        loss += Lambda_L1*L1_loss\n",
    "                    loss = loss.mean()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    train_loss += loss\n",
    "#                     loss = None\n",
    "#                     train_loss += loss.item()\n",
    "\n",
    "                for idx in range(nb_batch_valid):\n",
    "                    model.eval()\n",
    "                    data = X_valid[idx*batch_size:(idx+1)*batch_size]\n",
    "                    target = Y_valid[idx*batch_size:(idx+1)*batch_size]\n",
    "                    tData = torch.DoubleTensor(data)\n",
    "                    tTarget = torch.DoubleTensor(target)\n",
    "                    tData, tTarget = tData.cuda(), tTarget.cuda() \n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(tData)\n",
    "                        loss = criterion(outputs, tTarget.reshape(-1,1))\n",
    "                        if Lambda_AUC > 0 :\n",
    "                            roc_train = roc_auc_score(tTarget.reshape(-1,1).clone().cpu().data, outputs.clone().cpu().data)\n",
    "                            AUC_loss = Lambda_AUC*((1-roc_train)/roc_train)\n",
    "                            loss += loss.clone()*AUC_loss\n",
    "                        L1_loss = 0\n",
    "                        if Lambda_L1 > 0 :\n",
    "                            for param in model.parameters():\n",
    "                                L1_loss += torch.sum(torch.abs(param))\n",
    "                            loss += Lambda_L1*L1_loss\n",
    "#                     valid_loss += loss.item()\n",
    "                    valid_loss += loss.mean()\n",
    "                Y_hat = predict([model], X_valid)\n",
    "                roc_valid = roc_auc_score(Y_valid, Y_hat)\n",
    "\n",
    "                model.cpu()\n",
    "                if valid_loss < valid_loss_local_value[model_idx]:\n",
    "                    valid_loss_local_value[model_idx] = valid_loss\n",
    "                    if valid_loss < valid_loss_global_value[model_idx]:\n",
    "                        valid_loss_global_value[model_idx] = valid_loss\n",
    "#                 else:\n",
    "#                     countdown[model_idx] -= 1\n",
    "\n",
    "                saved = \" \"\n",
    "                valid_metric = valid_loss\n",
    "                if P[\"valid_Metric\"] == \"auc\": valid_metric = (1-roc_valid)\n",
    "                if valid_metric < valid_metric_local_value[model_idx]:\n",
    "                    valid_metric_local_value[model_idx] = valid_metric\n",
    "                    countdown[model_idx] = countdown_init\n",
    "                    if valid_metric < valid_metric_global_value[model_idx]:\n",
    "                        found_a_better_model = True\n",
    "                        recaculate_roc = True                        \n",
    "                        valid_metric_global_value[model_idx] = valid_metric\n",
    "                        state_dict = model.state_dict()\n",
    "                        models_kept[model_idx].load_state_dict(state_dict)\n",
    "                        saved = \"*\"\n",
    "                else:\n",
    "                    countdown[model_idx] -= 1\n",
    "\n",
    "                if valid_loss < valid_loss_global:\n",
    "                    valid_loss_global = valid_loss \n",
    "\n",
    "                if roc_valid > roc_valid_value:\n",
    "                    roc_valid_value = roc_valid \n",
    "\n",
    "                if roc >= roc_value :\n",
    "                    roc_value = roc\n",
    "                \n",
    "                found_a_better_model_tag = \" \"\n",
    "                if found_a_better_model == True:\n",
    "                    found_a_better_model_tag = \"!\"\n",
    "\n",
    "                print('{}M:{}{} Epoch:{:3}  Tr: {:.4f}  Va: {:.5f} {:.5f} {:.5f}  auc {:.4f} {:.4f}  {}  {}'.format(\n",
    "                    found_a_better_model_tag, model_idx, saved, epoch, train_loss/nb_batch_train, \n",
    "                    valid_loss/nb_batch_valid, valid_loss_local_value[model_idx]/nb_batch_valid, \n",
    "                    valid_loss_global_value[model_idx]/nb_batch_valid, \n",
    "                    roc_valid, roc, countdown, len(X_train) ))\n",
    "\n",
    "        if recaculate_roc == True:\n",
    "            recaculate_roc = False\n",
    "            Y_hat = predict(models_kept, X)\n",
    "            roc = roc_auc_score(Y, Y_hat)\n",
    "\n",
    "        if sum(countdown) == 0:\n",
    "            break\n",
    "    \n",
    "    if found_a_better_model == False:\n",
    "        break\n",
    "    \n",
    "    if P_Boosting :\n",
    "        print(\"boosting...\")\n",
    "        for model_idx, model in enumerate(models_kept):\n",
    "            X_train, Y_train = [], []\n",
    "            X_valid, Y_valid = [], []\n",
    "            valid_selected = []\n",
    "            for j in range(P[\"data_NbValidFolds\"]):\n",
    "                valid_selected.append((model_idx+j)%len(models))\n",
    "                X_valid = XL[(model_idx+j)%len(models)]\n",
    "                Y_valid = YL[(model_idx+j)%len(models)]\n",
    "                if P_Boosting_Top_valid > 0 :\n",
    "                    Y_hat = predict([model], X_valid)\n",
    "                    diff = np.abs(np.array(Y_valid) - Y_hat.reshape(-1))\n",
    "                    indice_to_boost[(model_idx+j)%len(models)] += [i for i in (diff.argsort()[-P_Boosting_Top_valid:])]\n",
    "            train_selected = [j for j in range(len(models)) if j not in(valid_selected)]\n",
    "            for j in train_selected:\n",
    "                X_train = XL[(j)%len(models)]\n",
    "                Y_train = YL[(j)%len(models)]\n",
    "                if P_Boosting_Top_train > 0 :\n",
    "                    Y_hat = predict([model], X_train)\n",
    "                    diff = np.abs(np.array(Y_train) - Y_hat.reshape(-1))\n",
    "                    indice_to_boost[(j)%len(models)] += [i for i in (diff.argsort()[-P_Boosting_Top_train:])]\n",
    "\n",
    "print(\"done.... ({:.8f})\".format(roc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090868046590606\n"
     ]
    }
   ],
   "source": [
    "Y_hat = predict(models_kept, X)\n",
    "\n",
    "auc = roc_auc_score(Y, Y_hat)\n",
    "\n",
    "df_grid = pd.read_csv(\"grid.csv\", index_col=0)\n",
    "df_grid.loc[grid_index, \"auc\"] = auc\n",
    "df_grid.to_csv(\"grid.csv\", index=True)\n",
    "\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction_114.csv\n",
      "Prediction_114_M00.csv\n",
      "Prediction_114_M01.csv\n",
      "Prediction_114_M02.csv\n",
      "Prediction_114_M03.csv\n",
      "Prediction_114_M04.csv\n",
      "Prediction_114_M05.csv\n",
      "Prediction_114_M06.csv\n",
      "Prediction_114_M07.csv\n",
      "Prediction_114_M08.csv\n",
      "Prediction_114_M09.csv\n"
     ]
    }
   ],
   "source": [
    "Y_hat = predict(models_kept, Z)\n",
    "\n",
    "df_result = df_test[[\"ID_code\"]].copy()\n",
    "df_result[\"target\"] = pd.Series(Y_hat.reshape(-1))\n",
    "# df_result.head()\n",
    "filename = \"Prediction_{:03}.csv\".format(grid_index)\n",
    "df_result.to_csv(filename, index=False)\n",
    "print(filename)\n",
    "\n",
    "for model_idx, model in enumerate(models_kept):\n",
    "    Y_hat_m = predict([model], Z)\n",
    "    df_result = df_test[[\"ID_code\"]].copy()\n",
    "    df_result[\"target\"] = pd.Series(Y_hat_m.reshape(-1))\n",
    "    filename = \"Prediction_{:03}_M{:02}.csv\".format(grid_index, model_idx)\n",
    "    df_result.to_csv(filename, index=False)\n",
    "    print(filename)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 6.08M/6.08M [01:00<00:00, 106kB/s]\n",
      "Successfully submitted to Santander Customer Transaction Prediction"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c santander-customer-transaction-prediction -f Prediction_114_XGB.csv -m \"XGB only\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileName                 date                 description                        status    publicScore  privateScore  \r\n",
      "-----------------------  -------------------  ---------------------------------  --------  -----------  ------------  \r\n",
      "Prediction_114_XGB.csv   2019-04-06 08:06:53  XGB only                           complete  0.883        None          \r\n",
      "Prediction_114_Vote.csv  2019-04-06 08:05:16  vote 4 methods                     complete  0.894        None          \r\n",
      "Prediction_114.csv       2019-04-06 07:49:47  real downsampling                  complete  0.876        None          \r\n",
      "Prediction_113.csv       2019-03-17 00:54:28  all-in in-loop aug                 complete  0.890        None          \r\n",
      "Prediction_112.csv       2019-03-17 00:53:33  quick in-loop aug                  complete  0.888        None          \r\n",
      "Prediction_110.csv       2019-03-17 00:52:37  all-in +lgb +xgb over-overfitting  complete  0.872        None          \r\n",
      "Prediction_104.csv       2019-03-16 08:46:08  all-in                             complete  0.882        None          \r\n",
      "Prediction_097.csv       2019-03-16 08:45:10  gnb augmented                      complete  0.880        None          \r\n",
      "Prediction_094.csv       2019-03-16 08:44:06  mod_ActSlope 0.10                  complete  0.875        None          \r\n",
      "Prediction_093.csv       2019-03-15 07:03:16  mod_ActSlope 0.05                  complete  0.875        None          \r\n",
      "Prediction_092.csv       2019-03-15 07:01:26  train_LbdL2 0.002                  complete  0.870        None          \r\n",
      "Prediction_092.csv       2019-03-15 06:44:06  train_LbdL2 0.002                  complete  0.870        None          \r\n",
      "Prediction_091.csv       2019-03-14 12:38:29  LbdL1 0.00001                      complete  0.875        None          \r\n",
      "Prediction_088.csv       2019-03-14 12:37:33  LbdAUC 0.02                        complete  0.875        None          \r\n",
      "Prediction_087.csv       2019-03-13 16:09:26  hidden  400                        complete  0.874        None          \r\n",
      "Prediction_086.csv       2019-03-13 07:13:05  3 layers / upsample 0.1 - 0.914    complete  0.875        None          \r\n",
      "Prediction_083.csv       2019-03-13 07:12:09  3 layers - 0.9136                  complete  0.876        None          \r\n",
      "Prediction_078.csv       2019-03-12 14:21:21  batch size to 250                  complete  0.875        None          \r\n",
      "Prediction_076.csv       2019-03-12 14:14:16  batch size to 250                  complete  0.873        None          \r\n",
      "Prediction_076.csv       2019-03-12 11:59:37  momentum to 0.95                   complete  0.873        None          \r\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submissions -c santander-customer-transaction-prediction -q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
